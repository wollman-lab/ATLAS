"""TissueMultiGraph Analysis module.

The module contains the three main classes required graph based tissue analysis: 
TissueMultiGraph: the key organizing class used to create and manage graphs across layers
TissueGraph: object that represent a single biospatial unit tyoe
            Each layers (cells, zones, regions) is defined using 
            spatial and feature graphs that are both part ot a single tissuegraph
Taxonomy: a container class that stores information about the taxonomical units 
          (cell types, region types).  

Note
----
In current implementation each TMG object is stored as multiple files in a single directory. 
It is the same directory that stores input files used by TMG to create it's different objects. 
Only a single TMG object can exist in each directory. 
To create two TMG objects from the same data, just copy the raw data. 

"""

import pandas as pd
import numpy as np

import logging
import os.path
import json

import igraph
import pynndescent

from scipy.sparse.csgraph import dijkstra
import scipy.sparse

from multiprocessing import Pool

import anndata

from dredFISH.Utils import basicu
from dredFISH.Utils import tmgu
from dredFISH.Utils import geomu
from dredFISH.Utils import coloru
from dredFISH.Processing.Section import *
from dredFISH.Registration.Registration import *

rng = np.random.default_rng()

# define function (not nested...) to be used in parallel code
def create_and_save_geoms(sec, xy, basepath): 
    poly_dict = geomu.calc_mask_voronoi_polygons_from_XY(xy)
    sec_geoms = dict()
    for gt in ["mask","voronoi"]:
        sec_geoms[gt] = Geom(geom_type=gt, polys = poly_dict[gt],
                section = sec, basepath = basepath)
        sec_geoms[gt].save()

    return sec_geoms

class TissueMultiGraph: 
    """Main class used to manage the creation of multi-layer graph representation of tissues. 
    
    TissueMultiGraph (TMG) acts as a factory to create objects that represents the state of different biospatial units in tissue, 
    and their relationships. Examples of biospatial units are cells, isozones, regions. 
    The main objects that TMG creates and stores are TissueGraphs, Taxonomies, and Geoms.    

    Attributes
    ----------
    Layers : list 
        This is the main spatial/feature data storage representing biospatial units (cells, isozones, and regions)
        
    Taxonomies : list 
        The taxonomical representation of the different types biospatial units can have. 
        There is a many-to-one relationship between TissueGraphs Layers and Taxonomies. 
        Multiple layers can have the same taxnomy (cells and isozones both have the same taxonomy). 
        The Taxonomies store type related information (full names, feature_type_mats, relationship between types, etc). 
    
    Geoms : list of dicts
        List (one per section) of dict that contains Geom objects that represents geometrical aspects of the TMG required for Vizualization. 
        
    inputpath : str
        The top level path generated by Processing that has all the sections in it.

    basepath : str 
        Where the TMG data is saved
        
    layers_graph : list of tuples
        Stores relatioship between layers. 
        For example, [(1,2),(1,3)] inducates that layer 2 used layer 1 (isozones are build on cells) and that layer 3 (regions) uses layer 1. 
    
    layer_taxonomy_mapping : list of tuples
        Stores relationship between TG layers and Taxonomies
        
 """
    def __init__(self, 
        basepath = None,
        input_df = None,
        redo = False,
        skip_geoms = True,
        mem_only = False, 
        ):
        """Create a TMG object
        
        There could only be a single TMG object in basepath. 
        if one already exists (and redo is False) it will be loaded. If not a new empty one will be created. 
        
        Parameters
        ----------
            input_df : pandas dataframe with info about datafiles 
                       cols are: ['animal', 'section_acq_name', 'registration', 'processing', 'dataset',
                                  'dataset_path']

            basepth : a path to underwhith we create Analysis
                
            redo : bool (default False)
                If the object was already created in the past, the default behavior is to just load the object. 
                This can be overruled when redo is set to True. If this is the first time a TMG object is created 

            skip_geoms: set as False if you are not planning to do any plotting (for speed)
            
            mem_only: if you are not planning to save this object to file, setting mem_only to True will skip creating the folder for it

        """

        # check if file exist in basepath
        self.basepath = basepath

        if not redo and os.path.exists(os.path.join(self.basepath,"TMG.json")):
            self._load()
            return 
        
            # with open(os.path.join(self.basepath,"TMG.json"),encoding="utf-8") as fh:
            #     self._config = json.load(fh)
            
            # # Load Taxonomies: 
            # TaxNameList = self._config["tax_types"]
            # self.Taxonomies = [None]*len(TaxNameList)
            # for i in range(len(TaxNameList)): 
            #     self.Taxonomies[i] = Taxonomy(TaxNameList[i])
            #     self.Taxonomies[i].load(self.basepath,self.dataset)

            # # Load layers
            # # convert string key to int key (fixing an artifects of JSON dump and load)
            # ltm = self._config["layer_taxonomy_mapping"]
            # ltm = {int(layer_ix): tax_ix for layer_ix, tax_ix in ltm.items()}
            # self.layer_taxonomy_mapping = ltm 
            
            # LayerNameList = self._config["layer_types"]
            # self.Layers = [None]*len(LayerNameList)
            # for i in range(len(LayerNameList)): 
            #     self.Layers[i] = TissueGraph(basepath = self.basepath,
            #                                  dataset = self.dataset, 
            #                                  layer_type = LayerNameList[i], 
            #                                  redo = False)
                
            # self.layers_graph = self._config["layers_graph"]

            # # Load geoms
            # self.Geoms = [None] * len(self.unqS)
            # geom_types = self._config["geom_types"]
            # self.skip_geoms = skip_geoms
            # if not skip_geoms:
            #     for i,s in enumerate(self.unqS):
            #         section_geoms = dict()
            #         for gt in geom_types:
            #             polys = fileu.load(self.basepath,file_type='Geom',model_type=gt,
            #                                 section=s,dataset=self.dataset)
            #             section_geoms[gt] = Geom(geom_type=gt,polys=polys,
            #                                         basepath=self.basepath,
            #                                         section=s,dataset=self.dataset)
            #         self.Geoms[i] = section_geoms
            # else:
            #     section_geoms = dict() 
            #     for gt in geom_types:
            #         section_geoms[gt] = None
            #     self.Geoms[0] = section_geoms

            # return

        # create a new TMG object from inputs
        self.input_df = input_df
        if mem_only: 
            redo = True

        # create basepath if needed
        if not os.path.exists(self.basepath) and not mem_only:
            os.mkdir(self.basepath, mode = 0o775)

        self.TMG_ver = 1
        self.Layers = list() # a list of TissueGraphs
        self.layers_graph = list() # a list of tuples that keep track of the relationship between different layers 
        self.skip_geoms = skip_geoms
        self.Geoms = list()
            
        self.Taxonomies = list() # a list of Taxonomies
        self.layer_taxonomy_mapping = dict() # dict() # a dictopnary that keep tracks of which TissueGraph (index into Layer) 
                                                # uses which taxonomy (index into Taxonomies)
        # conf dict to map geoms to layer types
        self.geom_to_layer_type_mapping = {'total' : 'cell', 
                                            'nuclei' : 'cell', 
                                            'cytoplasm' : 'cell',
                                            'voronoi' : 'cell',
                                            'points' : 'cell',
                                            'isozones' : 'isozone', 
                                            'regions' : 'region'}
        
        self.layer_to_geom_type_mapping = {'cell' : 'voronoi',
                                            'isozone' : 'isozones',
                                            'region' : 'regions'}
        
        return 

    def _load(self,skip_geoms = False):
        with open(os.path.join(self.basepath,"TMG.json"),encoding="utf-8") as fh:
            self._config = json.load(fh)
        self.input_df = pd.DataFrame(self._config["input_dfs"])
        # Load Taxonomies: 
        TaxNameList = self._config["tax_types"]
        self.Taxonomies = [None]*len(TaxNameList)
        for i in range(len(TaxNameList)): 
            self.Taxonomies[i] = Taxonomy(TaxNameList[i])
            self.Taxonomies[i].load(self.basepath,self.dataset)

        # Load layers
        # convert string key to int key (fixing an artifects of JSON dump and load)
        ltm = self._config["layer_taxonomy_mapping"]
        ltm = {int(layer_ix): tax_ix for layer_ix, tax_ix in ltm.items()}
        self.layer_taxonomy_mapping = ltm 
        
        LayerNameList = self._config["layer_types"]
        self.Layers = [None]*len(LayerNameList)
        for i in range(len(LayerNameList)): 
            self.Layers[i] = TissueGraph(basepath = self.basepath,
                                            layer_type = LayerNameList[i], 
                                            redo = False)
            
        self.layers_graph = self._config["layers_graph"]

        # Load geoms
        self.Geoms = [None] * len(self.unqS)
        geom_types = self._config["geom_types"]
        self.skip_geoms = skip_geoms
        if not self.skip_geoms:
            self.load_geoms(sections=self.unqS,geom_types=geom_types)
        else:
            section_geoms = dict() 
            for gt in geom_types:
                section_geoms[gt] = None
            self.Geoms[0] = section_geoms

    def save(self):
        """ create the TMG.json and save everything.
        
        Saving simply iterates over all three types of objects (Layers, Taxonomies, Geom and call their respective save)
        Mapping between layers and layers/taxonomies are saved in a simple TMG.json file. 
        
        """

        # Geoms are a list of dicts (with identical keys)m get keys from first one. 
        if len(self.Geoms) > 0 and isinstance(self.Geoms[0],dict):
            geom_types = list(self.Geoms[0].keys())
        else:
            geom_types=list() 

        input_df_dict = self.input_df.to_dict('list')
        self._config = { "layers_graph" : self.layers_graph, 
                         "layer_taxonomy_mapping" : self.layer_taxonomy_mapping, 
                         "tax_types" : [tx.name for tx in self.Taxonomies], 
                         "layer_types" : [tg.layer_type for tg in self.Layers],
                         "geom_types" : geom_types,
                         "input_dfs" : input_df_dict}
        
        with open(os.path.join(self.basepath, "TMG.json"), 'w',encoding="utf-8") as json_file:
            json.dump(self._config, json_file)
           
        for i in range(len(self.Layers)): 
            if i == 0:
                _adata = self.Layers[0].adata
                _adata.obsm['XY'] = _adata.obsm['XY'].astype(np.float32)
            self.Layers[i].save()
        
        for i in range(len(self.Taxonomies)): 
            self.Taxonomies[i].save(self.basepath, self.dataset)

        if not self.skip_geoms: 
            for i in range(len(self.unqS)):
                for gt in geom_types:
                    if isinstance(self.Geoms[i][gt],Geom):
                        self.Geoms[i][gt].save()
              
        logging.info("saved")
        
    def load_geoms(self,sections = None, geom_types = ["mask","voronoi"]):
        # get section names 
        if sections is None: 
            sections = self.unqS
        elif not isinstance(sections,list):
            sections = [sections]

        for s in sections: 
            ix = self.unqS.index(s)
            section_geoms = dict()
            for gt in geom_types:
                section_geoms[gt] = Geom(geom_type=gt,polys=None,basepath=self.basepath, section=s)
            self.Geoms[ix] = section_geoms
    
    def add_type_information(self, layer_id, type_vec, tax): 
        """Adds type information to TMG
        
        Bookeeping method to add type information and update taxonomies etc. 
        
        Parameters
        ----------
        layer_id : int 
            what layers are we adding type info to? should be a `cell` layer 
        type_vec : numpy 1D array / list
            the integer codes of type 
        tax : int / str(Taxnomy.name) / Taxonomy
            either an integer that will be intepreted as an index to existing taxonomies or a Taxonomy object
        
        """
        if len(self.Layers) < layer_id or layer_id is None or layer_id < 0: 
            raise ValueError(f"requested layer id: {layer_id} doesn't exist")

        # if Tax is a new Taxonomy, add it to self
        if isinstance(tax, Taxonomy): # add to the pool and use an index to represent it
            self.Taxonomies.append(tax)
            tax_id = len(self.Taxonomies)-1

        if isinstance(tax,str):
            tax_names = [t.name for t in self.Taxonomies]
            tax_id = [i for i, word in enumerate(tax_names) if word == tax]
            if len(tax_id)==0:
                raise ValueError(f"taxonomy {tax} not found in {tax_names}")
            if len(tax_id)>1:
                raise ValueError(f"taxonomy {tax} apears more then once in {tax_names}, please check!")
            tax_id = tax_id[0]
            tax = self.Taxonomies[tax_id]

        if any(isinstance(item, str) for item in type_vec):
            type_vec = tax.get_type_ix(type_vec)
            
        # update .Type  
        self.Layers[layer_id].Type = type_vec

        # update mapping between layers and taxonomies to TMG (self)
        # this decides on which type to move on next...
        self.layer_taxonomy_mapping[layer_id] = tax_id 
        return 
    
    def create_cell_layer(self,  
                          norm='robust_regression', 
                          register_to_ccf = True,
                          metric='cosine',
                          build_spatial_graph = False,
                          build_feature_graph = False):
         
        """Creating cell layer from raw data. 
        
        Loads cell data for given section using their acq name (WellX_SectionY) 
        Cell layer is unique as it's the only one where spatial information is directly used with Voronoi
        
        Parameters
        ----------

         
        """
        allowed_options = ['logrowmedian','log','none','logregress','robust_regression']
        if norm not in allowed_options:
            raise ValueError(f"Choose norm param from {allowed_options}")
        
        for TG in self.Layers:
            if TG.layer_type == "cell":
                print("!!`cell` layer already exists; return...")
                return

        # find list of sections
        adatas = []
        sections = []
        for index,row in self.input_df.iterrows():
            animal = row['animal']
            section_acq_name = row['section_acq_name']
            dataset = row['dataset']
            registration_path = row['registration_path']
            processing = row['processing']
            dataset_path = row['dataset_path']
            try:
                adata = fileu.load(os.path.join(dataset_path,dataset,processing,section_acq_name),file_type='anndata')
                if register_to_ccf: 
                    XYZC  = Registration_Class(adata.copy(),registration_path,section_acq_name,verbose=False).run()
                    adata.obs['ccf_x'] = XYZC['ccf_x']
                    adata.obs['ccf_y'] = XYZC['ccf_y']
                    adata.obs['ccf_z'] = XYZC['ccf_z']
                    """ Rename Section """
                    section_name = f"{animal}_{adata.obs['ccf_x'].mean():.1f}"
                else: 
                    section_name = section_acq_name
                
                adatas.append(adata)
                sections.append(np.full((adata.shape[0], 1), section_name))
            except:
                print('Unable to load '+str(section_acq_name))

        adata = anndata.concat(adatas)
        print(f"{adata.shape[0]} cells across {adata.obs['section_index'].unique().shape[0]} sections")
        
        if register_to_ccf: 
            XY = np.array(adata.obs[["ccf_z","ccf_y"]])
        else: 
            XY = np.array(adata.obs[["stage_x","stage_y"]])
        S = np.vstack(sections)

        FISHbasis = adata.X.copy()
        adata.layers['raw'] = FISHbasis.copy()
        if norm == 'robust_regression':
            FISHbasis_norm = basicu.normalize_fishdata_robust_regression(FISHbasis)
        elif norm == 'logrowmedian':
            FISHbasis_norm = basicu.normalize_fishdata_logrowmedian(FISHbasis)
        elif norm == 'log':
            FISHbasis_norm = basicu.normalize_fishdata_log(FISHbasis)
        elif norm == 'logregress': 
            FISHbasis_norm = basicu.normalize_fishdata_log_regress(FISHbasis)
        else:
            FISHbasis_norm = FISHbasis
        adata.X = FISHbasis_norm

        TG = TissueGraph(adata=adata,
                         basepath = self.basepath,
                         layer_type="cell",
                         redo=True)

        # add observations and init size to 1 for all cells
        TG.node_size = np.ones((FISHbasis_norm.shape[0],))

        # add XY and section information 
        TG.XY = XY
        TG.Section = S

        # build two key graphs
        if build_spatial_graph:
            logging.info('building spatial graphs')
            TG.build_spatial_graph()
        if build_feature_graph:
            logging.info('building feature graphs')
            TG.build_feature_graph(FISHbasis_norm, metric=metric)
        
        # add layer
        self.Layers.append(TG)
        logging.info('done with create_cell_layer')
        return

    def create_isozone_layer(self, cell_layer = 0, replace = False):
        """Creates isozones layer using cell types. 
        Contract cell types to create isozone graph. 
        cell_layer determine the base layer to contract
        replace = True (default false) allow overwriting existing layer
        """
        isozone_layer_id = None
        for i,TG in enumerate(self.Layers):
            if TG.layer_type == "isozone":
                isozone_layer_id = i
                if not replace: 
                    raise ValueError("!!`isozone` layer already exists; change replace=True to overwrite")
         
        IsoZoneLayer = self.Layers[cell_layer].contract_graph()
        if isozone_layer_id is None: 
            self.Layers.append(IsoZoneLayer)
            isozone_layer_id = len(self.Layers)-1  
        else: 
            self.Layers[isozone_layer_id] = IsoZoneLayer

        self.layer_taxonomy_mapping[isozone_layer_id] = self.layer_taxonomy_mapping[cell_layer]
        self.layers_graph.append((cell_layer,isozone_layer_id))
    
    def create_region_layer(self, topics, region_tax, cell_layer=0):
        """Add region layor given cells' region types (topics)
        
        Parameters
        ----------
        topics : numpy array / list
            An array with local type environment (a number for each cell) 
        region_tax : Taxonomy
            A Taxonomy object that contains the region classification scheme
        cell_layer : int (default,0)
            Which layer in TMG is the cell layer?          

        """
        for TG in self.Layers:
            if TG.layer_type == "region":
                print("!!`region` layer already exists; return...")
                return 

        # create region layers through graph contraction
        CG = self.Layers[cell_layer].contract_graph(topics) # contract the cell layer by topics
        
        # contraction assumes that the feature_mat and taxonomy of the contracted layers are
        # inherited from the layer used for contraction. This is not true for regions so we need to update these
        # feature_mat is updated here and tax is updated by calling add_type_information
        Env = self.Layers[cell_layer].extract_environments(typevec=CG.Upstream)
        row_sums = Env.sum(axis=1)
        row_sums = row_sums[:,None]
        Env = Env/row_sums
        
        # create the region layer merging information from contracted graph and environments
        RegionLayer = TissueGraph(feature_mat=Env, basepath=self.basepath, 
                                  layer_type="region", redo=True)
        RegionLayer.SG = CG.SG.copy()
        RegionLayer.node_size = CG.node_size.copy()
        RegionLayer.Upstream = CG.Upstream.copy()
        RegionLayer.XY = CG.XY.copy()
        RegionLayer.Section = CG.Section.copy()

        self.Layers.append(RegionLayer)
        current_layer_id = len(self.Layers)-1
        self.add_type_information(current_layer_id, CG.Type, region_tax)

        # update the layers graph to show that regions are created from cells
        self.layers_graph.append((cell_layer, current_layer_id))

    def fill_holes(self,lvl_to_fill,min_node_size):
        """EXPERIMENTAL: merges small biospatial units with their neighbors. 

        The goal of this method is to allow filling up "holes", i.e. chunks in the tissue graph that we 
        are unhappy about their type using local neighbor types. 

        Note
        ----
        As of 6/8/2022 this method is not ready for production use. 
        """
        update_feature_mat_flag=False
        if self.Layers[lvl_to_fill].feature_mat is not None: 
            feature_mat = self.Layers[lvl_to_fill].feature_mat.copy()
            update_feature_mat_flag = True

        # get types (with holes)
        region_types = self.Layers[lvl_to_fill].Type.copy()

        # mark all verticies with small node size
        region_types[self.Layers[lvl_to_fill].node_size < min_node_size]=-1
        fixed = region_types > -1

        # renumber region types in case removing some made numbering discontinuous
        _,ix = np.unique(region_types[fixed],return_inverse=True)
        cont_region_types = region_types.copy()
        cont_region_types[fixed] = ix

        # map to level-0 to do the label propagation at the cell level: 
        fixed_celllvl = self.map_to_cell_level(lvl_to_fill,fixed)
        contregion_celllvl = self.map_to_cell_level(lvl_to_fill,cont_region_types)

        # fill holes through label propagation
        lblprp = self.Layers[0].SG.community_label_propagation(weights=None, initial=contregion_celllvl, fixed=fixed_celllvl)
        region_type_filled = np.array(lblprp.membership)

        # shrink indexes from level-0 to requested level-1 
        _,ix = self.map_to_cell_level(lvl_to_fill-1,return_ix=True)
        _, ix_zero_to_two = np.unique(ix, return_index=True)
        new_type = region_type_filled[ix_zero_to_two]

        # recreate the layer
        NewLayer = self.Layers[lvl_to_fill-1].contract_graph(new_type)
        self.Layers[lvl_to_fill] = NewLayer
        if update_feature_mat_flag:
            self.Layers[lvl_to_fill].feature_mat = feature_mat[fixed,:]

    def find_upstream_layer(self, layer_id):
        """
        We addume that cell is always 0 ("root"). 
        #TODO: deal with cases where the upstream layer is not cell (future layers beyong isozone / regions) 
        """
        upstream_layer_id=0
        return upstream_layer_id
    
    def map_to_cell_level(self, lvl, VecToMap=None, return_ix=False):
        """
        Maps values to first layer of the graph, mostly used for plotting. 
        lvl is the level we want to map all the way to 

        TODO: this function might have some problems
        """
        # if VecToMap is not supplied, will use the layer Type as default thing to map to lower levels
        if VecToMap is None: # type 
            VecToMap = self.Layers[lvl].Type.astype(np.int64)
        if isinstance(VecToMap, str) and VecToMap == 'index': 
            VecToMap = np.arange(self.Layers[lvl].N)
        elif len(VecToMap) != self.Layers[lvl].N:
            raise ValueError("Number of elements in VecToMap doesn't match requested Layer size")
            
        # if needed (i.e. not already at cell level) expand indexing backwards in layers following layers_graph
        if lvl == 0:
            return VecToMap
        else:  # (lvl > 0)
            # # while lvl > 0:
            # lvl = self.find_upstream_layer(lvl)
            # ix=ix[self.Layers[lvl].Upstream

            ix = self.Layers[lvl].Upstream
            VecToMap = VecToMap[ix].flatten()
            if return_ix: 
                return (VecToMap,ix)
            else: 
                return VecToMap
  
    @property
    def N(self):
        """list : Number of cells in each layer of TMG."""
        return([L.N for L in self.Layers])
    
    def get_N(self, section=None): 
        return([L.get_N(section=section) for L in self.Layers])
    
    @property
    def Ntypes(self):
        """list : Number of types in each layer of TMG."""
        return([L.Ntypes for L in self.Layers])

    @property
    def layer_types(self):
        """list : layer_type of each layer in TMG"""
        return([L.layer_type for L in self.Layers])
    
    def find_layer_by_name(self, layer_type):
        all_layer_types = np.array(self.layer_types)
        ix = np.flatnonzero(all_layer_types == layer_type)
        if len(ix) == 0:
            print(f"No layer of type {layer_type} was found.")
            print("Check layer_types to see what options already exist in a TMG object") 
            return(ix)
        if len(ix) != 1: 
            raise ValueError("More then one layer has the same name, please check")
        return(ix[0])


    @property
    def Nsections(self):
        """int : Number of unique sections"""
        return(len(self.unqS))

    @property
    def unqS(self):
        assert len(self.Layers)
        Sections = self.Layers[0].Section
        # return a list of (unique) sections 
        return(list(np.unique(Sections)))
    
    def add_and_save_vor_mask_geoms(self):
        XY_per_section=self.Layers[0].get_XY(section=self.unqS)
        XY_per_section_dict = {s: xy for s, xy in zip(self.unqS, XY_per_section)}
        args = [(sec, XY_per_section_dict[sec],self.basepath) for sec in self.unqS]
        with multiprocessing.Pool() as pool:
            self.Geoms = pool.starmap(create_and_save_geoms, args)


    def add_geoms(self,geom_types = ["mask","voronoi"], unqS = None,redo = False):
        """
        Creates the geometries needed (mask, lines, points, and polygons) to be used in views to create maps.
        Geometries are vectorized representations of cells using lists of Shapely objects. 
        Supported Geoms: 
        * mask
        * voronoi
        * cells
        * nuclei 
        * cytoplasm 
        * isozones 
        * regions 

        There are some reports that shaeply 2.0 is better, but conda fails to install it. This workaround could be avoided
        in the future either by upgrading shaply or some other workaround. 
        """

        self.skip_geoms = False
        # create XY per section 
        XY_per_section = list()
        if unqS is None: 
            unqS = self.unqS
        for s in unqS:
            XY_per_section.append(self.Layers[0].get_XY(section=s))

        # if mask and/or voronoi are part of the geom_type, run those calculations in parallel
        geoms_requiering_voronoi = ['mask','voronoi','isozones','regions']
        
        if any(gtype in geoms_requiering_voronoi for gtype in geom_types): 
            with Pool() as pool: 
                section_geom_polys = pool.map(geomu.calc_mask_voronoi_polygons_from_XY,XY_per_section)

        # Init Geom
        self.Geoms = list()

        for i,s in enumerate(unqS):
           # init all polygons for each section
            masked_vor_polys = section_geom_polys[i]['voronoi']
            mask_polys = section_geom_polys[i]['mask']

            # Initalize an empty Geom dict for this section
            section_geoms = {}

            if "mask" in geom_types:
                section_geoms['mask'] = Geom(geom_type='mask',polys = mask_polys,section = s,
                                            basepath = self.basepath)
            if "voronoi" in geom_types:
                section_geoms['voronoi'] = Geom(geom_type='voronoi',polys = masked_vor_polys,section = s,
                                                basepath = self.basepath)
            
            # Vectorize cell / nuclei / cytoplasm segmentations matrix
            if "cells" in geom_types:
                cell_labels_raster = self.load_stiched_labeled_image(section = s,label_type = 'total') 
                cell_polys = geomu.vectorize_labeled_matrix_to_polygons(cell_labels_raster)
                section_geoms['cells'] = Geom(geom_type='cells',polys = cell_polys,section = s,
                                              basepath = self.basepath)
                
            if "nuclei" in geom_types:
                nuclei_labels_raster = self.load_stiched_labeled_image(section = s,label_type = 'nuclei')
                nuclei_polys = geomu.vectorize_labeled_matrix_to_polygons(nuclei_labels_raster)
                section_geoms['nuclei'] = Geom(geom_type='nuclei',polys = nuclei_polys,section = s,
                                               basepath = self.basepath)
                
            if "cytoplasm" in geom_types:
                cytoplasm_labels_raster = self.load_stiched_labeled_image(section = s,label_type = 'cytoplasm')
                cytoplasm_polys = geomu.vectorize_labeled_matrix_to_polygons(cytoplasm_labels_raster)
                section_geoms['cytoplasm'] = Geom(geom_type='cytoplasm',polys = cytoplasm_polys,section = s,
                                                  basepath = self.basepath)

            # add "higher level" polygon merging based on layer mapping
            if "isozones" in geom_types:
                layer_ix = self.find_layer_by_name(self.geom_to_layer_type_mapping['isozones'])
                zones_polys = geomu.merge_polygons_by_ids(masked_vor_polys,self.Layers[layer_ix].Upstream)
                section_geoms['isozones'] = Geom(geom_type='isozones',polys = zones_polys,section = s,
                                                  basepath = self.basepath)
                
            if "regions" in geom_types: 
                layer_ix = self.find_layer_by_name(self.geom_to_layer_type_mapping['regions'])
                region_polys = geomu.merge_polygons_by_ids(masked_vor_polys,self.Layers[layer_ix].Upstream)
                section_geoms['regions'] =Geom(geom_type='regions',polys = region_polys,section = s,
                                               basepath = self.basepath)

            self.Geoms.append(section_geoms)
        


    def load_stiched_labeled_image(self,section = '',label_type = 'total',flip = True):
        if section is None:
            section = self.unqS[0]
        
        # load from drive using fileu
        lbl = fileu.load(os.path.join(self.inputpath,section),file_type='mask',model_type = label_type)
        lbl = lbl.numpy()
        # zero out any labels in the mask do not mattch the TG names
        # does that by finding layers, getting names, subsetting to section
        names = self.Layers[0].names
        Sections = self.Layers[0].Section
        ix = np.flatnonzero(Sections == section)
        names = names[ix]
        # find all unique labels in the lbl matrix
        i, j = np.nonzero(lbl)
        unq_lbl = np.unique(lbl[i,j])
        unq_lbl_flt = np.copy(unq_lbl)
        unq_names = np.unique(names)
        ix_flt = np.logical_not(np.isin(unq_lbl_flt,unq_names))
        unq_lbl_flt[ix_flt]=0
        
        # zeros out labels that are not in names
        lookup_o2n = pd.Series(unq_lbl_flt, index=unq_lbl)
        lbl_filtered = geomu.swap_mask(lbl, lookup_o2n)

        if flip:
            lbl_filtered = np.transpose(lbl_filtered) 

        return(lbl_filtered)

class TissueGraph:
    """Representation of transcriptional state of biospatial units as a graph. 
    
    TissueGraph (TG) is the core class used to analyze tissues using Graph representation. 
    TG stores a two layer graph G = {V,Es,Ef} where Es are spatial edges (physical neighbors) and Ef are feature neighnors. 
    TG stores information on position (XYS) and features that used to created these graphs using anndata object representation.  
    Each TG has a reference to a Taxonomy object that contains information on the types ( 
    
    Note
    ----
    TissueGraph objects are typically not created on their own, but using method calls of TMG (create_{cell,isozones,regions}_layer)
    
    Attributes
    ----------
        tax : Taxonomy
            a Taxonomy object that contain labels, type stats, and relationship between types. 
        SG : iGraph
            Graph representation of the spatial relationship between biospatiual units (i.e. cells, zones, regions). 
            SG might include multiple componennts for multiple section data and is non-weighted graph (1 - neighbors, 0 not neighbors). 
        FG : iGraph 
            Graph representation of feature similarity between biospatial units.  
               
        
    main methods:
        * contract_graph - find zones/region, i.e. spatially continous areas in the graph the same (cell/microenvironment) type
        * cond_entopy - calculates the conditional entropy of a graph given types (or uses defaults existing types)
        * watershed - devide into regions based on watershed


    """
    def __init__(self, basepath=None, layer_type=None,
                       feature_mat=None, feature_mat_raw=None,
                       redo=False, obs=None,layers=None,
                       adata=None
        ):
        """Create a TissueGraph object
        
        Parameters
        ----------
        feature_mat : numpy 2D arrary or tuple of matrix size
            Matrix of the spatial units features (expression, composition, etc). 
            As alternative to the full matrix, input could be a tuple of matrix size (samples x features)
        basepath : str
            Where to read/write files related to this TG
        layer_type : str
            Name of the type of layer (cells, isozones, regions)
        """
        self.allowed_layer_types = ['cell', 'isozone', 'region']

        # validate input
        if basepath is None: 
            raise ValueError("missing basepath in TissueGraph constructor")
        if layer_type is None: 
            raise ValueError("Missing layer type information in TissueGraph constructor")
        if layer_type not in self.allowed_layer_types:
            raise ValueError(f"Layer type {layer_type} is not in {self.allowed_layer_types}")

        # what is stored in this layer (cells, zones, regions, etc)
        self.layer_type = layer_type # label layers by their type
        self.basepath = basepath
 
        # this dict stores the defaults field names in the anndata objects that maps to TissueGraph properties
        # this allows storing different versions (i.e. different cell type assignment) in the anndata object 
        # while still maintaining a "clean" interfact, i.e. i can still call for TG.Type and get a type vector without 
        # knowing anything about anndata. 
        # To see where in AnnData everything is stored, check comment in rows below 
        self.adata_mapping = {"Type": "Type", #obs
                              "node_size": "node_size", #obs
                              "name" : "label", #obs
                              "XY" : "XY", #obsm
                              "Section" : "Slice"} #obs
        # Note: a these mapping are not used for few attributes such as SG/FG/Upstream that are "hard coded" 
        # as much as possible, the only memory footprint is in the anndata object, the exceptions are SG/FG that 
        # are large objects that we want to keep as iGraph in mem. Therefore, SG/FG are created during init from adata.obsp
        
        # Key graphs - spatial and feature based
        self.SG = None # spatial graph (created by build_spatial_graph, or load in __init__)
        self.FG = None # Feature graph (created by build_feature_graph, or load in __init__)
        self._spatial_edge_list = None # for performance (of .contract_graph), going to save the edge list extermally from self.SG 

        # there are two mode of TG loading: 
        # 1. standard (from drive): load the full adata and create SG and FG 
        # 2. from scratch : uses input arguments to rebuild the TG object from scratch, ignores anything in the drive. 
        
        if not redo:
            self.adata = anndata.read_h5ad(os.path.join(self.basepath,'Layer',f"{self.layer_type}_layer.h5ad"))
            # SG is saved as a list of spatial graphs (one per section)
            if "SG" in self.adata.obsp.keys():
                # create SG and FG from Anndata
                sg = self.adata.obsp["SG"] # csr matrix
                self.SG =  tmgu.adjacency_to_igraph(sg, directed=False, simplify = False)
                
            # FG - there is one feature graph for the whole TG object
            if "FG" in self.adata.obsp.keys():
                fg = self.adata.obsp["FG"] # csr matrix
                self.FG = tmgu.adjacency_to_igraph(fg, directed=False, simplify = False)
            
        else: # create an object from given feature_mat data

            # if feautre_mat is a tuple, replace with an empty sparse matrix
            if isinstance(feature_mat,tuple): 
                feature_mat = scipy.sparse.csr_matrix(feature_mat)

            # The main data container is an anndata, initalize with feature_mat
            if adata is not None:
                self.adata = adata.copy()
            elif feature_mat is None: 
                self.adata = anndata.AnnData(feature_mat, obs=obs)
            else: 
                self.adata = anndata.AnnData(feature_mat, obs=obs,dtype=feature_mat.dtype) # the tissuegraph AnnData object

            if feature_mat_raw is not None:
                self.adata.layers['raw'] = feature_mat_raw

        if layers!=None:
            self.adata.layers = layers

        return None
    
    def is_empty(self):
        """Determines if the TG object is empty
        
        Checks if internal adata is None of empty
        """ 
        if self.adata is None or self.adata.shape[0]==0: 
            return True
        else: 
            return False

    def filter(self,logical_vec,rebuild_SG = True, rebuild_FG = True): 
        """
        removes observations from TG. 
        can only work if layer_type==cells
        """
        if self.layer_type != "cell": 
            raise ValueError("can only filter at the cell level")

        self.adata = self.adata[logical_vec]

        # rebuild igraph layers (that are only saved as adj matrix within anndata)
        if rebuild_SG and self.SG != None: 
            self.build_spatial_graph()
        else:
            # after filtering, need to rebuild SG, if it existed (and rebuild_SG was False) zeros it out
            self.SG = None
        
        if rebuild_FG and self.FG != None: 
            self.build_feature_graph()
        else: 
            # after filtering, need to rebuild FG, if it existed (and rebuild_FG was False) zeros it out
            self.FG = None

    def save(self):
        """save TG to file"""
        layer_path = os.path.join(self.basepath, 'Layer')
        if not os.path.exists(layer_path):
            os.makedirs(layer_path)
        if not self.is_empty():
            self.adata.write(os.path.join(layer_path,f"{self.layer_type}_layer.h5ad"))

    
    @property
    def names(self):
        """list : observation names"""
        if self.is_empty():
            return None
        return self.adata.obs[self.adata_mapping["name"]]
    
    def get_names(self,section = None):
        if section is None: 
            return(self.names)
        else:
            return(self.names[self.Section == section])

    @names.setter
    def names(self,names):
        self.adata.obs[self.adata_mapping["name"]]=names
    
    @property
    def Upstream(self):
        """list : mapping between current TG layer (self) and upstream layer
        Return value has the length of upstream level and index values of current layer""" 
        if self.is_empty():
            return None
        # Upstream is stored as uns in adata: 
        return self.adata.uns["Upstream"]
    
    @Upstream.setter
    def Upstream(self,V):
        self.adata.uns["Upstream"]=V
    
    @property
    def spatial_edge_list(self):
        if self._spatial_edge_list is None: 
            if self.SG is None: 
                raise ValueError('cannot return edge list before spatial graph was created')
            self._spatial_edge_list = np.array(self.SG.get_edgelist(),dtype=int)

        return self._spatial_edge_list.copy()

    @property
    def feature_mat_shape(self):
        return self.adata.shape

    @property
    def feature_mat(self):
        """matrix : the feature values for this TG observations
        
        The feature_mat is stored in the underlying anndata object and is required to properly init it. 
        """
        # if adata is still None, return None
        if self.is_empty():
            return None
        # otherwide, feature_mat is stored as the main data in adata
        return(self.adata.X.copy())
    
    @feature_mat.setter
    def feature_mat(self,X):
        self.adata.X = X
    
    def get_feature_mat(self,section = None):
        if section is None: 
            return(self.feature_mat.copy())
        else: 
            return(self.feature_mat[self.Section == section,:])

    @property 
    def Type(self): 
        """Type
        """
        if self.is_empty():
            return None
        elif self.adata_mapping["Type"] not in self.adata.obs.columns.values.tolist(): 
            return None
            # raise ValueError("Mapping of type to AnnData is broken, please check!")
        else:
            typ = self.adata.obs[self.adata_mapping["Type"]]
            typ = np.array(typ) 
            return typ
        
    @Type.setter
    def Type(self,Type):
        """list : list (or 1D np array) of integer values that reference a Taxonomy object types""" 
        self.adata.obs[self.adata_mapping["Type"]] = Type
        
    @property
    def N(self):
        """int : Size of the tissue graph
            internally stored as igraph size
        """
        if not self.is_empty():
            return(self.adata.shape[0])
        else: 
            raise ValueError('TissueGraph does not contain an AnnData object, please verify!')
    
    @property
    def node_size(self):
        if self.is_empty():
            return None
        elif self.adata_mapping["node_size"] not in self.adata.obs.columns.values.tolist(): 
            raise ValueError("Mapping of type to AnnData is broken, please check!")
        else: 
            return self.adata.obs[self.adata_mapping["node_size"]]
    
    @node_size.setter
    def node_size(self,Nsz):
        self.adata.obs[self.adata_mapping["node_size"]] = list(Nsz)
    
    @property
    def Section(self):
        """
            Section : dependent property - will query info from anndata and return
        """
        if self.adata is None:
            return None
        elif self.adata_mapping["Section"] not in self.adata.obs.columns.values.tolist(): 
            raise ValueError("Mapping of type to AnnData is broken, please check!")
        else: 
            return self.adata.obs[self.adata_mapping["Section"]]

    @property
    def unqS(self):
        Sections = self.Section
        # return a list of (unique) section
        return(list(np.unique(Sections)))

    @property
    def size_of_sections(self):
        _,count = np.unique(self.Section,return_counts = True)
        return(count)

    @Section.setter
    def Section(self,Section):
        self.adata.obs[self.adata_mapping["Section"]]=Section

    @property
    def XY(self):
        """
            XY : dependent property - will query info from anndata and return
        """
        if self.adata is None:
            return None
        elif self.adata_mapping["XY"] not in self.adata.obsm.keys(): 
            raise ValueError("Mapping of XY to AnnData is broken, please check!")
        else: 
            return self.adata.obsm[self.adata_mapping["XY"]]

    def get_XY(self,section = None):
        if section is None: 
            return(self.XY)
        elif isinstance(section, list):
            return [self.XY[self.Section == sec, :] for sec in section]
        else: 
            return(self.XY[self.Section == section,:])
        
    def get_N(self, section=None): 
        if section is None: 
            return self.N
        else: 
            return np.sum(self.Section==section)

    @XY.setter
    def XY(self,XY): 
        self.adata.obsm[self.adata_mapping["XY"]]=XY
        
    @property    
    def X(self):
        """
            X : dependent property - will query info from Graph and return
        """
        return(self.XY[:,0])
        
    @property
    def Y(self):
        """Y : dependent property - will query info from Graph and return
        """
        return(self.XY[:,1])
    
    @property    
    def Ntypes(self): 
        """ 
            Ntypes: returns number of unique types in the graph
        """ 
        if self.Type is None: 
            raise ValueError("Type not yet assigned, can't count how many")
        return(len(np.unique(self.Type)))
    
    @property
    def Nsections(self):
        """
            Nsections : returns number of unique sections in TG
        """
        unqS = np.unique(self.Section)
        return(len(unqS))
    
    def build_feature_graph(self, 
        X = None, n_neighbors=15, metric=None, accuracy={'prob':1, 'extras':1.5}, metric_kwds={}, return_graph=False):
        """construct k-graph based on feature similarity

        Create a kNN graph (an igraph object) based on feature similarity. The core of this method is the calculation on how to find neighbors. 
        If metric is "precomputed" the distances are assumed to be known and we're almost done. 
        For all other metric values, we use pynndescent 

        Parameters
        ----------
        X : numpy array
            Either a distance matrix, i.e. squareform(pdist(feature_mat)) if metric = 'precomputed'.  ) or just a feature_mat
            by defaults (if it's None) will use self.feature_mat
        n_neighbors : int
            How many neighbors (k) should we use in the knn graph
        metric : str
            either "precomputed", "random", or one of the MANY metrics supported by pynndescent. Random is for debugging only. 
        accuracy : dict with fields: 'prob' and 'extras'
            a dictionary with accuracy options for pynndescent. 'prob' should be in [0,1] and 'extras' is typically >1. 
            accuracy['prob'] conrols the 'diversify_prob' and accuracy['extra'] the 'pruning_degree_multplier' 
        metric_kwds : dict
            passthrough kwds that will be sent to pynndescent. 
        return_graph : bool
            will return the graph instead of updating self.FG

        Note
        ----
        There are LOTS of metric implemnted in pynndescent. 
        Many are not updated in the readthedocs so check the sources code! 
        """
    
        logging.info(f"building feature graph using {metric}")
        if metric is None:
            raise ValueError('metric was not specified')

        # If X is none, use feature_mat
        if X is None: 
            X = self.feature_mat

        # checks if we have enough rows 
        n_neighbors = min(X.shape[0]-1,n_neighbors)

        if metric == 'precomputed':
            indices = np.argsort(X,axis=1)
            distances = np.sort(X,axis=1)
        elif metric == 'random': 
            indices = np.random.randint(X.shape[0],size=(X.shape[0],n_neighbors+1))
            distances = np.ones((X.shape[0],n_neighbors+1))
        else:
            # perform nn search (using accuracy x number of neighbors to improve accuracy)
            knn = pynndescent.NNDescent(X,n_neighbors = n_neighbors,
                                          metric = metric,
                                          diversify_prob = accuracy['prob'],
                                          pruning_degree_multiplier = accuracy['extras'],
                                          metric_kwds = metric_kwds)

            # get indices and remove self. 
            (indices,distances) = knn.neighbor_graph

        # take the first K values remove first self similarities    
        indices = indices[:,1:]
        distances = distances[:,1:]

        id_from = np.tile(np.arange(indices.shape[0]),indices.shape[1])
        id_to = indices.flatten(order='F')

        # build graph
        edgeList = np.vstack((id_from,id_to)).T
        G = igraph.Graph(n=X.shape[0], edges=edgeList)
        G.simplify()

        # register
        self.adata.obsp["FG"] = G.get_adjacency_sparse()
        self.FG = G
        if return_graph:
            return G
    
    def build_spatial_graph(self,max_dist = 300):
        """construct graph based on Delaunay neighbors
        
        build_spatial_graph will create an igrah using Delaunay triangulation
        
        Spatial graph can potentially be a multi-component one. Cells cannot be neighbors if they are in different sections
        or or they are more than max_dist away from each other. 

        """
        unqS = self.unqS
        logging.info(f"Building spatial graphs for {self.Nsections} sections")
        
        self.SG = list()
        for s in range(self.Nsections): 
            # get XY for a given section
            XY_per_section = self.XY[self.Section==unqS[s],:]
            self.SG.append(geomu.spatial_graph_from_XY(XY_per_section,max_dist=max_dist))
            
        # to merge the spatial graphs into one with many components: 
        self.SG = igraph.Graph.disjoint_union(self.SG[0],self.SG[1:])
        
        logging.info("updating anndata")
        self.adata.obsp["SG"] = self.SG.get_adjacency_sparse()
        self.adata.obs[self.adata_mapping["node_size"]] = np.ones(self.XY.shape[0])
        logging.info("done building spatial graph")
    
    def contract_graph(self, TypeVec=None, return_useful_layer = True):
        """find zones/region, i.e. spatially continous areas in the graph the same (cell/microenvironment) type
        
        reduce graph size by merging spatial neighbors of same type. 
        Given a vector of types, will contract the graph to merge vertices that are both next to each other and of the same type. 
        
        to deal with sections, i.e. discontinous spatial graphs, it first merges the SG list into a large grpah 
        does all the calculations, and then splits it again. 

        Parameters
        ----------
        TypeVec : 1D numpy array with dtype int (default value is self.Type)
            a vector of Types for each node. If None, will use self.Type

        Note
        ----
        Default behavior is to assign the contracted TG the same taxonomy as the original graph. 
        
        Returns
        -------
        TissueGraph 
            A TG object after vertices merging. 
        """

        # Figure out which type to use
        if TypeVec is None: 
            TypeVec = self.Type

        # convert to int if needed
        if isinstance(TypeVec[0],str): 
            _,TypeVec = np.unique(TypeVec,return_inverse = True)

        # Merge the spatial graphs from multiple sections into one large 
        
        # get edge list - note that Spatial graphs work with indexes not cell names      
        EL = self.spatial_edge_list
        
        # only keep edges where neighbors are of same types
        EL = EL[np.take(TypeVec,EL[:,0]) == np.take(TypeVec,EL[:,1]),:]
        
        # remake a graph with potentially many components
        IsoZonesGraph = igraph.Graph(n=self.N, edges=EL, directed = False)

        # commented next line for perfornace as it shouldn't be needed
        # the input graph is simnple and we're only removing edges
        #IsoZonesGraph = IsoZonesGraph.as_undirected().simplify()

        # because we used both type and proximity, the original graph (based only on proximity)
        # that was a single component graph will be broken down to multiple components 
        # finding clusters for each component. 
        cmp = IsoZonesGraph.components()
        
        IxMapping = np.asarray(cmp.membership)
        
        ZoneName, ZoneSingleIx, ZoneSize = np.unique(IxMapping, return_index=True, return_counts = True)
        
        # calculate zones feature_mat
        # if it's an empty sparse matrix of if np.ndarray with all values are Nan, 
        # just replace with tuple of the required size
        if not return_useful_layer:
            zone_feat_mat = (len(ZoneSize),0)
        elif scipy.sparse.issparse(self.feature_mat) and self.feature_mat.nnz==0:
            zone_feat_mat = (len(ZoneSize),self.feature_mat_shape[1])
        elif isinstance(self.feature_mat,np.ndarray) and np.all(np.isnan(self.feature_mat)):
            zone_feat_mat = (len(ZoneSize),self.feature_mat_shape[1])
        else: 
            df = pd.DataFrame(data = self.feature_mat)
            df['type']=IxMapping
            zone_feat_mat = np.array(df.groupby(['type']).mean())
            
        # create new SG for zones 
        ZSG = self.SG.copy()
        ZSG.contract_vertices(IxMapping)
        ZSG.simplify()

        # create a new Tissue graph by copying existing one, contracting, and updating XY
        ZoneGraph = TissueGraph(feature_mat=zone_feat_mat, 
                                basepath=self.basepath,
                                layer_type="isozone",
                                redo=True,
                                )
        
        # recreate contracted graph as a TissueGraph
        if return_useful_layer:
            # use weighted bincount to calc XY fast
            newX = np.bincount(IxMapping,weights = self.XY[:,0]) / ZoneSize
            newY = np.bincount(IxMapping,weights = self.XY[:,1]) / ZoneSize
            new_XY = np.hstack((newX,newY))
        
            # assigne seciton name using unique indexes
            unq_ix = np.unique(IxMapping)
            new_XY = np.zeros((len(unq_ix),2))
            _,unq_ix = np.unique(IxMapping, return_index = True)
            new_section = self.Section[unq_ix]


            ZoneGraph.XY = new_XY
            ZoneGraph.Section = new_section
            

        ZoneGraph.SG = ZSG
        ZoneGraph.names = ZoneName
        ZoneGraph.node_size = ZoneSize
        ZoneGraph.Type = TypeVec[ZoneSingleIx]
        ZoneGraph.Upstream = IxMapping
        
        return(ZoneGraph)
                             
    def type_freq(self, max_val = None): 
        """return the catogorical probability for each type in TG
        
        Probabilities are weighted by the node_size
        """
        if self.Type is None: 
            raise ValueError("Type not yet assigned, can't count frequencies")
        
        if max_val is None: 
            max_val = np.max(self.Type)
        
        cnts = np.bincount(self.Type, weights=self.node_size, minlength=max_val+1) 
        Ptypes = cnts / np.sum(cnts) 

        
        return Ptypes
    
    def cond_entropy(self,return_all = False):
        """calculate conditional entropy of the tissue graph
           
           cond entropy is the difference between graph entropy based on zones and type entropy
        """
        Pzones = self.node_size
        Pzones = Pzones/np.sum(Pzones)
        Entropy_Zone = -np.sum(Pzones*np.log2(Pzones))
        
        # validate that type exists
        if self.Type is None: 
            raise ValueError("Can't calculate cond-entropy without Types, please check")
            
        Ptypes = self.type_freq()
        Ptypes = Ptypes[Ptypes>0]
        Entropy_Types=-np.sum(Ptypes*np.log2(Ptypes))
        
        cond_entropy = Entropy_Zone-Entropy_Types
        if return_all: 
            return (Entropy_Zone,Entropy_Types,cond_entropy)
        else: 
            return(cond_entropy)
    
    def extract_environments(self,ordr = None,typevec = None):
        """returns the categorical distribution of neighbors. 
        
        Depending on input there could be two uses, 
            usage 1: if ordr is not None returns local neighberhood defined as nodes up to distance ordr on the graph for all vertices. 
            usage 2: if typevec is not None returns local env based on typevec, will return one env for each unique type in typevec
            
        Return
        ------
        numpy array
            Array with Type frequency for all local environments for all types in TG. 
        """
        unqlbl = np.unique(self.Type)
        
        # arrange the indexes for the environments. 
        # if we use ordr this is neighborhood defined by iGraph
        # if we provide types, than indexes of each type. 
        if ordr is not None and typevec is None:
            ind = self.SG.neighborhood(order = ordr)
        elif typevec is not None and ordr is None:
            ind = list()
            for i in range(len(np.unique(typevec))):
                ind.append(np.flatnonzero(typevec==i))
        else: 
            raise ValueError('either order or typevec must be provided, not both (or none)')
        
        unqlbl = np.unique(self.Type)
        Env = np.zeros((len(ind),len(unqlbl)),dtype=np.int64)
        ndsz = self.node_size.copy().astype(np.int64)
        int_types = self.Type.astype(np.int64)
        AllTypes = [int_types[ix] for ix in ind]
        AllSizes = [ndsz[ix] for ix in ind]
        for i in range(Env.shape[0]):
            Env[i,:]=np.bincount(AllTypes[i],weights = AllSizes[i],minlength=len(unqlbl))
        
        # should be the same as above, but much slower... keeping it here for now till more testing is done. 
        # for i in range(len(ind)):
        #     Env[i,:]=count_values(self.Type[ind[i]],unqlbl,ndsz[ind[i]],norm_to_one = False)
        return(Env)
    
    def graph_local_avg(self,VecToSmooth,ordr = 3,kernel = np.array([0.4,0.3,0.2,0.1])):
        """Simple local average of a Vec based on local neighborgood
        
        Parameters
        ----------
            VecToSmooth : numpy array
                The values we want to smooth. len(VecToSmooth) must be self.N
        """
        
        if len(VecToSmooth) is not self.N: 
            raise ValueError(f"Length of input vector {len(VecToSmooth)} doesn't match TG.N which is {self.N}")
        
        Smoothed = np.zeros((len(VecToSmooth),ordr+1))
        Smoothed[:,0] = VecToSmooth
        for i in range(ordr):
            ind = self.SG.neighborhood(order = i+1,mindist=i)
            for j in range(len(ind)):
                ix = np.array(ind[j],dtype=np.int64)
                Smoothed[j,i+1] = np.nanmean(VecToSmooth[ix])

        kernel = kernel[None,:]
        kernel = np.repeat(kernel,len(VecToSmooth),axis=0)
        ix_nan = np.isnan(Smoothed)
        kernel[ix_nan]=0
        sum_of_rows = kernel.sum(axis=1)
        kernel = kernel / sum_of_rows[:, None]
        Smoothed[ix_nan] = 0
        Smoothed = np.multiply(Smoothed,kernel)
        Smoothed = Smoothed.sum(axis=1)
        return(Smoothed)
    
    def watershed(self,InputVec):
        """Watershed segmentation based on InputVec values
        
        Watershed on the TG spatial graph. 
        First finds local peaks and then assigns all nodes to their closest local peak using dijkstra
        
        Parameters
        ----------
        
        Return
        ------
        (id,dist) 
            tuple with id and distance to cloest zone. 
        """
        is_peak = np.zeros(InputVec.shape).astype('bool')
        ind = self.SG.neighborhood(order = 1,mindist=1)
        for i in range(len(ind)):
            is_peak[i] = np.all(InputVec[i]>InputVec[ind[i]])
        peaks = np.flatnonzero(is_peak)  

        Adj = self.SG.get_adjacency_sparse()
        Dij_min, predecessors, ClosestPeak = dijkstra(Adj, directed=False, 
                                                          indices=peaks, 
                                                          return_predecessors=True, 
                                                          unweighted=False, 
                                                          limit=np.inf, 
                                                          min_only=True)
        
        # renumber all closest peak continously
        u,ix_rev = np.unique(ClosestPeak, return_inverse=True)
        ClosestPeak=u[ix_rev]
        
        # relabel HoodId in case we have a heterozone that was split along the way
        # by contracting and expanding where each contracted zone gets a unique ID we
        # basically garantuee that Ntypes = N (for now...)
        CG = self.contract_graph(TypeVec = ClosestPeak)
        Id = np.arange(CG.N)
        ClosestPeak = Id[CG.Upstream]
        return (ClosestPeak, Dij_min)
        
    def calc_entropy_at_different_Leiden_resolutions(self,Rvec = np.logspace(-1,2.5,100)): 
        Ent = np.zeros(Rvec.shape)
        Ntypes = np.zeros(Rvec.shape)
        for i in range(len(Rvec)):
            print(f"iter: {i}")
            TypeVec = self.FG.community_leiden(resolution_parameter=Rvec[i],objective_function='modularity').membership
            TypeVec = np.array(TypeVec).astype(np.int64)
            Ent[i] = self.contract_graph(TypeVec,return_useful_layer = False).cond_entropy()
            Ntypes[i] = len(np.unique(TypeVec))
            
        self.cond_entropy_df = pd.DataFrame(data = {'Entropy' : Ent, 'Ntypes' : Ntypes, 'Resolution' : Rvec})  
    
    def gradient_magnitude(self,V):
        """Spatial gradient based on spatial graph
        
        Calculate the gradient defined as sqrt(dV/dx^2+dV/dy^2) where dV/dx(y) is calcualted using simple trigo
        
        """
        EL = self.SG.get_edgelist()
        EL = np.array(EL,dtype='int')
        XY = self.XY
        dV = V[EL[:,1]]-V[EL[:,0]]
        dX = XY[EL[:,1],0]-XY[EL[:,0],0]
        dY = XY[EL[:,1],1]-XY[EL[:,0],1]                      
        alpha = np.arctan(dX/dY)
        dVdX = dV*np.cos(alpha)/dX
        dVdY = dV*np.sin(alpha)/dY 
        dVdXY = np.sqrt(dVdX**2 + dVdY**2)
        df = pd.DataFrame({'dVdXY' : np.hstack((dVdXY,-dVdXY))})
        df['type']=np.hstack((EL[:,0],EL[:,1]))
        gradmag = np.array(df.groupby(['type']).mean())
        gradmag = np.array(gradmag)
        return(gradmag)
    
    def graph_local_median(self,VecToSmooth,ordr = 3):
        """local median of VecToSmooth based in spatial graph
        """
        ind = self.SG.neighborhood(order = ordr)
        Smoothed = np.zeros(VecToSmooth.shape)
        for j in range(len(ind)):
            ix = np.array(ind[j],dtype=np.int64)
            Smoothed[j] = np.nanmedian(VecToSmooth[ix])
        return(Smoothed)

class Taxonomy:
    """Taxonomical system for different biospatial units (cells, isozones, regions)
    
    Attributes
    ----------
    name : str
        a name of the taxonomy (will be used to file io)
    
    """
    
    def __init__(self, name=None, 
        Types=None, feature_mat=None,rgb_codes=None
        ): 
        """Create a Taxonomy object

        Parameters
        ----------
        name : str
            the name of the taxonomy object
        Types : list 
            list of types names that will be used in this Taxonomy
        """
        if name is None: 
            raise ValueError("Taxonomy must get a name")
        self.name = name
        self._df = pd.DataFrame()

        if Types is not None and feature_mat is not None: 
            self.add_types(Types,feature_mat)

        # add RGB values if provided
        if rgb_codes is not None:
            self._df['RGB'] = list(rgb_codes)

        return None
    
    @property
    def Type(self): 
        """list: Building blocks of this Taxonomy
        
        Setter verify that there are no duplications. 
        """
        return(list(self._df.index))
    
    @Type.setter
    def Type(self,Types):
        if len(Types) is not len(set(Types)):
            dups = [Types.count(t) for t in set(Types) if Types.count(t)>1]
            raise ValueError(f"Types must be unique. Found duplicates: {dups}")
        if self.is_empty():
            self._df.index = Types
        else: 
            if len(Types) is not self._df.shape[0]: 
                raise ValueError('Changing Types of Taxonomy with defined values is not allowed')
            self._df.index = Types

    @property
    def feature_mat(self): 
        """ndarray: feature values for all types in the taxonomy
        
        Setter can get as input either numpy array (ordered same as self.Type) or pandas dataframe
        """
        if self.is_empty(): 
            return None
        else: 
            return(self._df.to_numpy())
        
    @feature_mat.setter
    def feature_mat(self,F):
        # first, make sure F is a DataFrame
        if isinstance(F, pd.DataFrame):
            df_features = F
        else: # assumes F is a matrix, make into a df and give col names
            df_features = pd.DataFrame(F)
            df_features.columns = [f"f_{i:03d}" for i in range(F.shape[1])]

        self._df = df_features
    #    # either replace or concat columns based on their name
    #     for c in df_features.columns: 
    #         if c in self._feature_cols: 
    #             self._df.loc[:,c]=F.loc[:,c]
    #         else: 
    #             self._df = pd.concat((self._df,df_features.loc[:,c]),axis=1)
    #             self._feature_cols.append(c)
        
    #     self._feature_cols = df_features.columns 
        
    def is_empty(self):
        """ determie if taxonomy is empty
        """
        return (self._df.empty)
        
    def save(self, basepath,dataset):
        """save to basepath using Taxonomy name
        """
        if not self.is_empty():
            fileu.save(self._df,path=basepath,
                                  file_type='Taxonomy',
                                  model_type=self.name,
                                  dataset=dataset)
                
    def load(self, basepath,dataset):
        """save from basepath using Taxonomy name
        """
        self._df = fileu.load(path=basepath,
                              file_type='Taxonomy',
                              model_type=self.name,
                              dataset=dataset)
        self._df.set_index('type', inplace=True)
        
    def add_types(self,new_types,feature_mat):
        """add new types and their feature average to the Taxonomy
        """
        df_new = pd.DataFrame(feature_mat)
        df_new['type']=new_types
        type_feat_df = df_new.groupby(['type']).mean()
        if self._df.empty:
            self._df = type_feat_df
        else: 
            missing_index = type_feat_df.index.difference(self._df.index)
            self._df = pd.concat((self._df,type_feat_df.iloc[missing_index,:]),axis=0)
        
        return None
    
    @property
    def N(self):
        return len(self.Type)

    @property
    def RGB(self):
        """
        Returns RGB values for all types (random value if none assigned)
        """
        if 'RGB' in self._df.columns:
            rgb = list(self._df['RGB'])
            if isinstance(rgb[0],str): 
                # if RGB is string, convert to tuples of 0-255
                rgb = coloru.hex_to_rgb(rgb)
                rgb = np.array(rgb)
        else: 
            rgb = coloru.rand_hex_codes(self._df.shape[0])
        return rgb

    @RGB.setter
    def RGB(self,rgb_codes):
        self._df['RGB'] = rgb_codes

    def get_type_ix(self,typ_str):
        Type_ix_dict = {item: i for i, item in enumerate(self.Type)}
        index_positions = [Type_ix_dict.get(item) for item in typ_str]
        return index_positions

class Geom:
    """
    Container class for a collection of polygons. 
    These geometires could be used to represent the units in a TissueGraphs but other uses are possible. 
    
    Supported Geoms: 
        * voronoi
        * cells
        * nuclei 
        * cytoplasm 
        * isozones 
        * regions 

        NOTE (1/25/2023): apperantly, extracting coordinates from 100K polygons in shapely is pretty slow
        therefore, during construction, doing this calculation once and caching it as _vert. 
        A wrap-around property, vert, serves that. If this could be faster, might make sense to calculate that on the fly. 

        For the vertices representation. Verts is a tuple of (ext,int) one for each polygon. Ext has one polygon and int a list of 
        potentially many "holes". The verts are computed by get_polygons_vertices and by keeping them in memory we are avoiding
        recomputing this every time the geomtry is used (mostly for plotting). This does add one-time cost during 
        __init__ to create this cached precomputed representation. 

    """

    def __init__(self,geom_type = None,polys = None,basepath = None,section = None, redo = False):
        self.type = geom_type
        self.allowed_geom_types=['voronoi',
                                  'cells',
                                  'nuclei',
                                  'cytoplasm',
                                  'isozones',
                                  'regions',
                                  'mask']
        if geom_type not in self.allowed_geom_types:
            raise ValueError(f"Geom type {geom_type} is not in {self.allowed_geom_types}")
        self.geom_type = geom_type
        self.section = section
        self.basepath = os.path.join(basepath,'Geom',section)
        self.filename = os.path.join(self.basepath, f"{self.geom_type}.wkt")
        # read from drive if exists already
        if polys is None and os.path.exists(self.filename): 
            polys = fileu.load_polygon_list(self.filename)
        self.polys = polys
        self._verts = geomu.get_polygons_vertices(polys)


    @property
    def verts(self):
        return self._verts

    def save(self):
        if not os.path.exists(self.basepath):
            os.makedirs(self.basepath)
        fileu.save_polygon_list(self.polys,self.filename)

       
    def get_xylim(self):
        """Returns the min/max xy of the geometry
        """
        # get xy of exterior coordinates
        xy = np.vstack(self.verts[0])
        # get limits
        xlim = [xy[:,0].min(), xy[:,0].max()]
        ylim = [xy[:,1].min(), xy[:,1].max()]
        return xlim,ylim
        

    