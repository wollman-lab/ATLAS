"""TissueMultiGraph Analysis module.

The module contains the three main classes required graph based tissue analysis: 
TissueMultiGraph: the key organizing class used to create and manage graphs across layers (hence multi)
TissueGraph: Each layers (cells, zones, regions) is defined using spatial and feature graphs that are both part ot a single tissuegraph
Taxonomy: a container class that stores information about the taxonomical units (cell types, region types).  

Note
----
In current implementation each TMG object is stored as multiple files in a single directory. 
That is the same directory that stores all the input files used by TMG to create it's different objects. 
Only a single TMG object can exist in each directory. To create two TMG objects from the same data, just copy the raw data. 

Example
-------
TMG is created in a folder with csv file(s) that have cells transcriptional state (\*_matrix.csv) and other metadata information such as x,y, section information in (\*_metadata.csv) files. 
The creation of TMG follows the methods to create different layers (create_cell_layer, create_zone_layer, create_region_layer) and creating Geoms using create_geoms. 
"""

# dependencies
from cmath import nan
import functools
from textwrap import indent
# import ipyparallel as ipp
from collections import Counter
import numpy as np
rng = np.random.default_rng()
import pandas as pd
import itertools
import logging
import os.path 
import glob
import json
import pickle

import igraph
import pynndescent 

from scipy.sparse.csgraph import dijkstra
from scipy.ndimage import distance_transform_edt
import scipy.sparse 

import anndata
# to create geomtries
from shapely.geometry import Polygon, MultiPolygon, LineString, MultiLineString, MultiPoint
from scipy.spatial import Delaunay, Voronoi
import shapely.wkt

from dredFISH.Utils import basicu 
from dredFISH.Utils import tmgu
from dredFISH.Utils import geomu
from dredFISH.Utils import coloru

from dredFISH.Processing.Section import *

import re

# for debuding mostly
import warnings
import time
# from IPython import embed

class TissueMultiGraph: 
    """Main class used to manage the creation of multi-layer graph representation of tissues. 
    
    TissueMultiGraph (TMG) acts as a factory to create objects that represents the state of different biospatial units in tissue, 
    and their relationships. Examples of biospatial units are cells, isozones, regions. 
    The main objects that TMG creates and stores are TissueGraphs, Taxonomies, and Geoms.    

    
    Attributes
    ----------
    Layers : list 
        This is the main spatial/feature data storage representing biospatial units (cells, isozones, and regions)
        
    Taxonomies : list 
        The taxonomical representation of the different types biospatial units can have. 
        There is a many-to-one relationship between TissueGraphs Layers and Taxonomies. 
        Multiple layers can have the same taxnomy (cells and isozones both have the same taxonomy). 
        The Taxonomies store type related information (full names, feature_type_mats, relationship between types, etc). 
    
    Geoms : list of dicts
        List (one per section) of dict that contains Geom objects that represents geometrical aspects of the TMG required for Vizualization. 
        
    inputpath : str
        The top level path generated by Processing that has all the sections in it.

    basepath : str 
        Where the TMG data is saved
        
    layers_graph : list of tuples
        Stores relatioship between layers. 
        For example, [(1,2),(1,3)] inducates that layer 2 used layer 1 (isozones are build on cells) and that layer 3 (regions) uses layer 1. 
    
    layer_taxonomy_mapping : list of tuples
        Stores relationship between TG layers and Taxonomies
        
 """
    def __init__(self, 
        inputpath = None, 
        redo = False,
        quick_load_cell_obs = False,
        path_suffix = '_Analysis'
        ):
        """Create a TMG object
        
        There could only be a single TMG object in basepath. 
        if one already exists (and redo is False) it will be loaded. If not a new empty one will be created. 
        
        Parameters
        ----------
            inputpath : str
                The top level by generated by Processing that hold on Section information. 
                This will be used to generated an new basepath at the same level with "_Analysis"   
                This is the same folder where all additional TMG files will be stored. 
                
            redo : bool (default False)
                If the object was already created in the past, the default behavior is to just load the object. 
                This can be overruled when redo is set to True. If this is the first time a TMG object is created 
            
            quick_load_cell_obs : bool (default False)
                only load cell observation data calculated by Processing module, ignoring all other Layers / Geometries
        Raises
        ------
            input must exists (os.path.exists) or an error will be raised
            
        """
        
        if inputpath is None or not os.path.exists(inputpath):
            raise ValueError(f"Path {inputpath} doesn't exist")

        # infer the basepath for this TMG object and create it if doesn't exist
        self.path_suffix = path_suffix
        self.inputpath = inputpath
        (prefix,self.dataset) = os.path.split(os.path.normpath(inputpath))
        self.basepath = os.path.join(prefix,self.dataset+self.path_suffix)
        if not os.path.exists(self.basepath):
            os.mkdir(self.basepath, mode = 0o775)

        # check to see if a TMG.json file exists in that folder; if not, create an empty TMG. 
        if redo or not os.path.exists(os.path.join(self.basepath,"TMG.json")):
            self.Layers = list() # a list of TissueGraphs
            self.layers_graph = list() # a list of tuples that keep track of the relationship between different layers 
            
            self.Geoms = list()
            
            self.Taxonomies = list() # a list of Taxonomies
            self.layer_taxonomy_mapping = dict() # dict() # a dictopnary that keep tracks of which TissueGraph (index into Layer) 
                                                 # uses which taxonomy (index into Taxonomies)
                                                 
            
        elif quick_load_cell_obs:
            # load cell level attributes only -- faster
            with open(os.path.join(self.basepath,"TMG.json")) as fh:
                self._config = json.load(fh)
            LayerNameList = self._config["layer_types"]
            self.Layers = [None]*len(LayerNameList)
            self.Layers[0] = TissueGraph(basepath=self.basepath,
                                         dataset = self.dataset, 
                                         layer_type=LayerNameList[0], 
                                         tax=None,  
                                         redo=False,
                                         quick_load_cell_obs=True,
                                         )
            self.layers_graph= list()
            self.layer_taxonomy_mapping=dict()
            self.Taxonomies = list()

        else: 
            # load from drive
            with open(os.path.join(self.basepath,"TMG.json")) as fh:
                self._config = json.load(fh)
            
            # start with Taxonomies: 
            TaxNameList = self._config["tax_types"]
            self.Taxonomies = [None]*len(TaxNameList)
            for i in range(len(TaxNameList)): 
                self.Taxonomies[i] = Taxonomy(TaxNameList[i])
                self.Taxonomies[i].load(self.basepath,self.dataset)
                    
            # convert string key to int key (fixing an artifects of JSON dump and load)
            ltm = self._config["layer_taxonomy_mapping"]
            ltm = {int(layer_ix): tax_ix for layer_ix, tax_ix in ltm.items()}
            self.layer_taxonomy_mapping = ltm 
            
            LayerNameList = self._config["layer_types"]
            self.Layers = [None]*len(LayerNameList)
            for i in range(len(LayerNameList)): 
                self.Layers[i] = TissueGraph(basepath = self.basepath,
                                             dataset = self.dataset, 
                                             layer_type = LayerNameList[i], 
                                             redo = False)
                
            self.layers_graph = self._config["layers_graph"]
            geom_types = self._config["geom_types"]
            self.Geoms = list()
            for s in self.unqS:
                section_geoms = dict()
                for gt in geom_types:
                    polys = fileu.load(self.basepath,type='Geom',model_type=gt,
                                       section=s,dataset=self.dataset)
                    section_geoms[gt] = Geom(geom_type=gt,polys=polys,
                                             basepath=self.basepath,
                                             section=s,dataset=self.dataset)
                self.Geoms.append(section_geoms)
        # conf dict to map geoms to layer types
        self.geom_to_layer_type_mapping = {'total' : 'cell', 
                                            'nuclei' : 'cell', 
                                            'cytoplasm' : 'cell',
                                            'voronoi' : 'cell',
                                            'points' : 'cell',
                                            'isozones' : 'isozone', 
                                            'regions' : 'region'}
        self.layer_to_geom_type_mapping = {'cell' : 'voronoi',
                                            'isozone' : 'isozones',
                                            'region' : 'regions'}

        return None
    
    def save(self):
        """ create the TMG.json and save everything.
        
        Saving simply iterates over all three types of objects (Layers, Taxonomies, Geom and call their respective save)
        Mapping between layers and layers/taxonomies are saved in a simple TMG.json file. 
        
        """

        # Geoms are a list of dicts (with identical keys)m get keys from first one. 
        if len(self.Geoms) > 0 and isinstance(self.Geoms[0],dict):
            geom_types = list(self.Geoms[0].keys())
        else:
            geom_types=list() 

        self._config = {"layers_graph" : self.layers_graph, 
                       "layer_taxonomy_mapping" : self.layer_taxonomy_mapping, 
                       "tax_types" : [tx.name for tx in self.Taxonomies], 
                       "layer_types" : [tg.layer_type for tg in self.Layers],
                       "geom_types" : geom_types}
        
        with open(os.path.join(self.basepath, "TMG.json"), 'w') as json_file:
            json.dump(self._config, json_file)

           
        for i in range(len(self.Layers)): 
            if i == 0:
                _adata = self.Layers[0].adata
                _adata.obsm['XY'] = _adata.obsm['XY'].astype(np.float32)
            self.Layers[i].save()
        
        for i in range(len(self.Taxonomies)): 
            self.Taxonomies[i].save(self.basepath, self.dataset)

        for i,s in enumerate(self.unqS):
            for j,gt in enumerate(geom_types):
                self.Geoms[i][gt].save()
                
                    
        logging.info(f"saved")
        
    def add_type_information(self, layer_id, type_vec, tax): 
        """Adds type information to TMG
        
        Bookeeping method to add type information and update taxonomies etc. 
        
        Parameters
        ----------
        layer_id : int 
            what layers are we adding type info to? should be a `cell` layer 
        type_vec : numpy 1D array / list
            the integer codes of type 
        tax : int / Taxonomy
            either an integer that will be intepreted as an index to existing taxonomies or a Taxonomy object
        
        """
        if len(self.Layers) < layer_id or layer_id is None or layer_id < 0: 
            raise ValueError(f"requested layer id: {layer_id} doesn't exist")

        # if Tax is a new Taxonomy, add it to self
        if isinstance(tax, Taxonomy): # add to the pool and use an index to represent it
            self.Taxonomies.append(tax)
            tax_id = len(self.Taxonomies)-1

        if isinstance(tax,str):
            tax_names = [t.name for t in self.Taxonomies]
            tax_id = [i for i, word in enumerate(tax_names) if word == tax]
            if len(tax_id)==0:
                raise ValueError(f"taxonomy {tax} not found in {tax_names}")
            if len(tax_id)>1:
                raise ValueError(f"taxonomy {tax} apears more then once in {tax_names}, please check!")
            tax_id = tax_id[0]
            tax = self.Taxonomies[tax_id]

        if any(isinstance(item, str) for item in type_vec):
            type_vec_inds = -np.ones(len(type_vec))
            for i,typ in enumerate(tax.Type): 
                type_vec_inds[type_vec==typ]=i
            type_vec = type_vec_inds

        # update .Type  
        self.Layers[layer_id].Type = type_vec

        # update mapping between layers and taxonomies to TMG (self)
        # this decides on which type to move on next...
        self.layer_taxonomy_mapping[layer_id] = tax_id 
        return 
    
    def create_cell_layer(self, metric='cosine', norm='default', 
                          norm_cell=True, norm_basis=True,
                          measurement_type = "total", 
                          hybes = [],
                          build_spatial_graph = True,build_feature_graph = True): 
        """Creating cell layer from raw data. 
        TODO: Fix documentaion after finishing Taxonomy class. 
        
        Cell layer is unique as it's the only one where spatial information is directly used with Voronoi
        
        Parameters
        ----------
            
        path_to_raw_data - path to folder with all raw-data (multiple sections)
        celltypes_org - labels for cells. 
        expand_types - id of cell types to expand. If they are all the last values, numbering will be continous 
        
         
        """
        allowed_options = ['default', 'logrowmedian','none']
        if norm not in allowed_options:
            raise ValueError(f"Choose from {allowed_options}")
        
        for TG in self.Layers:
            if TG.layer_type == "cell":
                print("!!`cell` layer already exists; return...")
                return

        if len(hybes)==0:
            for i in range(1,25):
                hybes.append(f"hybe{i:.0f}")

        # find list of sections
        section_names = os.listdir(self.inputpath)
        dfall_meta = []
        matrix_all = []
        for s in section_names:
            section_path = os.path.join(self.inputpath,s)
            meta = fileu.load(section_path,type='metadata',model_type=measurement_type)
            matrix = fileu.load(section_path,type='matrix',model_type=measurement_type)
            matrix = np.array(matrix[hybes])
            dfall_meta.append(meta)
            matrix_all.append(matrix)
        dfall_meta = pd.concat(dfall_meta)
        FISHbasis = np.vstack(matrix_all)
        XY = np.array(dfall_meta[["stage_x","stage_y"]])
        S =  np.array(dfall_meta[["section_index"]])

        if norm == 'default':
            # FISH basis is the raw count matrices from imaging; normalize data
            FISHbasis_norm = basicu.normalize_fishdata(FISHbasis, norm_cell=norm_cell, norm_basis=norm_basis)
        elif norm == 'logrowmedian':
            FISHbasis_norm = basicu.normalize_fishdata_logrowmedian(FISHbasis, norm_basis=norm_basis)
        else:
            FISHbasis_norm = FISHbasis
        
        # creating first layer - cell tissue graph
        TG = TissueGraph(feature_mat=FISHbasis_norm,
                         feature_mat_raw=FISHbasis,
                         basepath=self.basepath,
                         dataset = self.dataset,
                         layer_type="cell", 
                         obs=dfall_meta, # metadata carries over
                         redo=True)
        
        # add observations and init size to 1 for all cells
        TG.node_size = np.ones((FISHbasis_norm.shape[0],))

        # add XY and section information 
        TG.XY = XY
        TG.Section = S

        # build two key graphs
        if build_spatial_graph:
            logging.info('building spatial graphs')
            TG.build_spatial_graph()
        if build_feature_graph:
            logging.info('building feature graphs')
            TG.build_feature_graph(FISHbasis_norm, metric=metric)
        
        # add layer
        self.Layers.append(TG)
        logging.info('done with create_cell_layer')
        return

    def create_isozone_layer(self, cell_layer = 0):
        """Creates isozones layer using cell types. 
        Contract cell types to create isozone graph. 
        
        """
        for TG in self.Layers:
            if TG.layer_type == "isozone":
                print("!!`isozone` layer already exists; return...")
                return 
        IsoZoneLayer = self.Layers[cell_layer].contract_graph()
        self.Layers.append(IsoZoneLayer)
        layer_id = len(self.Layers)-1                             
        self.layer_taxonomy_mapping[layer_id] = self.layer_taxonomy_mapping[cell_layer]
        self.layers_graph.append((cell_layer,layer_id))
    
    def create_region_layer(self, topics, region_tax, cell_layer=0):
        """Add region layor given cells' region types (topics)
        
        Parameters
        ----------
        topics : numpy array / list
            An array with local type environment (a number for each cell) 
        region_tax : Taxonomy
            A Taxonomy object that contains the region classification scheme
        cell_layer : int (default,0)
            Which layer in TMG is the cell layer?          

        """
        for TG in self.Layers:
            if TG.layer_type == "region":
                print("!!`region` layer already exists; return...")
                return 

        # create region layers through graph contraction
        CG = self.Layers[cell_layer].contract_graph(topics) # contract the cell layer by topics
        
        # contraction assumes that the feature_mat and taxonomy of the contracted layers are
        # inherited from the layer used for contraction. This is not true for regions so we need to update these
        # feature_mat is updated here and tax is updated by calling add_type_information
        Env = self.Layers[cell_layer].extract_environments(typevec=CG.Upstream)
        row_sums = Env.sum(axis=1)
        row_sums = row_sums[:,None]
        Env = Env/row_sums
        
        # create the region layer merging information from contracted graph and environments
        RegionLayer = TissueGraph(feature_mat=Env, basepath=self.basepath, 
                                  dataset = self.dataset, layer_type="region", redo=True)
        RegionLayer.SG = CG.SG.copy()
        RegionLayer.node_size = CG.node_size.copy()
        RegionLayer.Upstream = CG.Upstream.copy()
        RegionLayer.XY = CG.XY.copy()
        RegionLayer.Section = CG.Section.copy()

        self.Layers.append(RegionLayer)
        current_layer_id = len(self.Layers)-1
        self.add_type_information(current_layer_id, CG.Type, region_tax)

        # update the layers graph to show that regions are created from cells
        self.layers_graph.append((cell_layer, current_layer_id))

    def fill_holes(self,lvl_to_fill,min_node_size):
        """EXPERIMENTAL: merges small biospatial units with their neighbors. 

        The goal of this method is to allow filling up "holes", i.e. chunks in the tissue graph that we 
        are unhappy about their type using local neighbor types. 

        Note
        ----
        As of 6/8/2022 this method is not ready for production use. 
        """
        update_feature_mat_flag=False
        if self.Layers[lvl_to_fill].feature_mat is not None: 
            feature_mat = self.Layers[lvl_to_fill].feature_mat.copy()
            update_feature_mat_flag = True

        # get types (with holes)
        region_types = self.Layers[lvl_to_fill].Type.copy()

        # mark all verticies with small node size
        region_types[self.Layers[lvl_to_fill].node_size < min_node_size]=-1
        fixed = region_types > -1

        # renumber region types in case removing some made numbering discontinuous
        unq,ix = np.unique(region_types[fixed],return_inverse=True)
        cont_region_types = region_types.copy()
        cont_region_types[fixed] = ix

        # map to level-0 to do the label propagation at the cell level: 
        fixed_celllvl = self.map_to_cell_level(lvl_to_fill,fixed)
        contregion_celllvl = self.map_to_cell_level(lvl_to_fill,cont_region_types)

        # fill holes through label propagation
        lblprp = self.Layers[0].SG.community_label_propagation(weights=None, initial=contregion_celllvl, fixed=fixed_celllvl)
        region_type_filled = np.array(lblprp.membership)

        # shrink indexes from level-0 to requested level-1 
        _,ix = self.map_to_cell_level(lvl_to_fill-1,return_ix=True)
        _, ix_zero_to_two = np.unique(ix, return_index=True)
        new_type = region_type_filled[ix_zero_to_two]

        # recreate the layer
        upstream_layer_ix = self.find_upstream_layer(lvl_to_fill)
        NewLayer = self.Layers[lvl_to_fill-1].contract_graph(new_type)
        self.Layers[lvl_to_fill] = NewLayer
        if update_feature_mat_flag:
            self.Layers[lvl_to_fill].feature_mat = feature_mat[fixed,:]

    def find_upstream_layer(self, layer_id):
        """
        We addume that cell is always 0 ("root"). 
        #TODO: deal with cases where the upstream layer is not cell (future layers beyong isozone / regions) 
        """
        upstream_layer_id=0
        return upstream_layer_id
    
    def map_to_cell_level(self, lvl, VecToMap=None, return_ix=False):
        """
        Maps values to first layer of the graph, mostly used for plotting. 
        lvl is the level we want to map all the way to 

        TODO: this function might have some problems
        """
        # if VecToMap is not supplied, will use the layer Type as default thing to map to lower levels
        if VecToMap is None: # type 
            VecToMap = self.Layers[lvl].Type.astype(np.int64)
        if isinstance(VecToMap, str) and VecToMap == 'index': 
            VecToMap = np.arange(self.Layers[lvl].N)
        elif len(VecToMap) != self.Layers[lvl].N:
            raise ValueError("Number of elements in VecToMap doesn't match requested Layer size")
            
        # if needed (i.e. not already at cell level) expand indexing backwards in layers following layers_graph
        if lvl == 0:
            return VecToMap
        else:  # (lvl > 0)
            # # while lvl > 0:
            # lvl = self.find_upstream_layer(lvl)
            # ix=ix[self.Layers[lvl].Upstream

            ix = self.Layers[lvl].Upstream
            VecToMap = VecToMap[ix].flatten()
            if return_ix: 
                return (VecToMap,ix)
            else: 
                return VecToMap
  
    @property
    def N(self):
        """list : Number of cells in each layer of TMG."""
        return([L.N for L in self.Layers])
    
    @property
    def Ntypes(self):
        """list : Number of types in each layer of TMG."""
        return([L.Ntypes for L in self.Layers])

    @property
    def layer_types(self):
        """list : layer_type of each layer in TMG"""
        return([L.layer_type for L in self.Layers])
    
    def find_layer_by_name(self, layer_type):
        all_layer_types = np.array(self.layer_types)
        ix = np.flatnonzero(all_layer_types == layer_type)
        if len(ix) == 0:
            print(f"No layer of type {layer_type} was found.")
            print("Check layer_types to see what options already exist in a TMG object") 
            return(ix)
        if len(ix) != 1: 
            raise ValueError("More then one layer has the same name, please check")
        return(ix[0])


    @property
    def Nsections(self):
        """int : Number of unique sections"""
        return(len(self.unqS))

    @property
    def unqS(self):
        assert len(self.Layers)
        Sections = self.Layers[0].Section
        # return a list of (unique) sections 
        return(list(np.unique(Sections)))

    def add_geoms(self,geom_types = ["points","mask","voronoi","cells"],mask_source = 'raster', unqS = None,redo = True):
        """
        Creates the geometries needed (mask, lines, points, and polygons) to be used in views to create maps.
        Geometries are vectorized representations of cells using lists of Shapely objects. 
        Supported Geoms: 
        * mask
        * voronoi
        * cells
        * nuclei 
        * cytoplasm 
        * isozones 
        * regions 

        There are some reports that shaeply 2.0 is better, but conda fails to install it. This workaround could be avoided
        in the future either by upgrading shaply or some other workaround. 
        """

        if redo == False and fileu.check_existance(path=self.basepath,type='Geom',model_type=geom_types[0],dataset=self.dataset,section=self.unqS[0]):
            self.Geoms = list()
            for s in self.unqS:
                section_geoms = dict()
                for gt in geom_types:
                    polys = fileu.load(self.basepath,type='Geom',model_type=gt,
                                       section=s,dataset=self.dataset)
                    section_geoms[gt] = Geom(geom_type=gt,polys=polys,
                                             basepath=self.basepath,
                                             section=s,dataset=self.dataset)
                self.Geoms.append(section_geoms)
            return
        
        # XY and Section infpormation is independent on Geoms
        allXY = self.Layers[0].XY
        Sections = self.Layers[0].Section
        if unqS is None: 
            unqS = self.unqS

        # init final voronoi and mask polys to None and created them first time they are used. this prevents 
        # creating something if we don't need it, and prevents us from creating it twice

        # Init Geom
        self.Geoms = list()

        def create_mask(self,mask_source,section,XY):
            if mask_source=='raster':
                cell_labels_raster = self.load_stiched_labeled_image(section = section,label_type = 'total')
                mask_polys = geomu.create_mask_from_raster(cell_labels_raster)
            elif mask_source=='XY':
                XYint = (XY*1000).astype(int)
                sz = np.fliplr(XYint).max(axis=0)+100
                cell_labels_raster = np.zeros(sz,dtype=bool)
                cell_labels_raster[XYint[:,1],XYint[:,0]]=True
                cell_labels_raster = distance_transform_edt(1-cell_labels_raster)<2
                mask_polys = geomu.create_mask_from_raster(cell_labels_raster)
                rescl = [0.001, 0, 0, 0.001, 0, 0]
                mask_polys = [shapely.affinity.affine_transform(m,rescl) for m in mask_polys]
            else: 
                raise ValueError(f"unsupported mask_source {mask_source}")
            return mask_polys

        # Create geoms per section: 
        for i,s in enumerate(unqS):
            # init all polygons for each section
            masked_vor_polys = None
            mask_polys = None
            cell_labels_raster = None

            print(f"started working on section {s}")
            # Initalize an empty Geom dict for this section
            section_geoms = {}
            
            # XY points are basis for many geoms:
            ix = np.flatnonzero(Sections==s) 
            XY = allXY[ix,:]

            # mask geom
            if "mask" in geom_types:
                mask_polys = create_mask(self,mask_source,s,XY)
                section_geoms['mask'] = Geom(geom_type='mask',polys = mask_polys,section = s,
                                             basepath = self.basepath,dataset = self.dataset)

            # Voronoi geometry 
            if "voronoi" in geom_types:
                if mask_polys is None:
                    mask_polys = create_mask(self,mask_source,s,XY)
                vor_polys = geomu.voronoi_polygons(XY)
                masked_vor_polys = geomu.mask_voronoi(vor_polys,mask_polys)
                section_geoms['voronoi'] = Geom(geom_type='voronoi',polys = masked_vor_polys,section = s,
                                                basepath = self.basepath,dataset = self.dataset)
            
            # Vectorize cell segmentations matrix
            if "cells" in geom_types:
                cell_labels_raster = self.load_stiched_labeled_image(section = s,label_type = 'total') 
                cell_polys = geomu.vectorize_labeled_matrix_to_polygons(cell_labels_raster)
                section_geoms['cells'] = Geom(geom_type='cells',polys = cell_polys,section = s,
                                              basepath = self.basepath,dataset = self.dataset)
                
            if "nuclei" in geom_types:
                nuclei_labels_raster = self.load_stiched_labeled_image(section = s,label_type = 'nuclei')
                nuclei_polys = geomu.vectorize_labeled_matrix_to_polygons(nuclei_labels_raster)
                section_geoms['nuclei'] = Geom(geom_type='nuclei',polys = nuclei_polys,section = s,
                                               basepath = self.basepath,dataset = self.dataset)
                
            if "cytoplasm" in geom_types:
                cytoplasm_labels_raster = self.load_stiched_labeled_image(section = s,label_type = 'cytoplasm')
                cytoplasm_polys = geomu.vectorize_labeled_matrix_to_polygons(cytoplasm_labels_raster)
                section_geoms['cytoplasm'] = Geom(geom_type='cytoplasm',polys = cytoplasm_polys,section = s,
                                                  basepath = self.basepath,dataset = self.dataset)

            if "isozones" in geom_types:
                if masked_vor_polys is None:
                    if mask_polys is None:
                        mask_polys = create_mask(self,mask_source,s,XY)
                    vor_polys = geomu.voronoi_polygons(XY)
                    masked_vor_polys = geomu.mask_voronoi(vor_polys,mask_polys)
                
                layer_ix = self.find_layer_by_name(self.geom_to_layer_type_mapping['isozones'])
                zones_polys = geomu.merge_polygons_by_ids(masked_vor_polys,self.Layers[layer_ix].Upstream)
                section_geoms['isozones'] = Geom(geom_type='isozones',polys = zones_polys,section = s,
                                                  basepath = self.basepath,dataset = self.dataset)
                
            if "regions" in geom_types: 
                if masked_vor_polys is None:
                    if mask_polys is None:
                        mask_polys = create_mask(self,mask_source,s,XY)
                    vor_polys = geomu.voronoi_polygons(XY)
                    masked_vor_polys = geomu.mask_voronoi(vor_polys,mask_polys)
                layer_ix = self.find_layer_by_name(self.geom_to_layer_type_mapping['regions'])
                region_polys = geomu.merge_polygons_by_ids(masked_vor_polys,self.Layers[layer_ix].Upstream)
                section_geoms['regions'] =Geom(geom_type='regions',polys = region_polys,section = s,
                                               basepath = self.basepath,dataset = self.dataset)

            self.Geoms.append(section_geoms)

    def load_stiched_labeled_image(self,section = '',label_type = 'total',flip = True):
        if section is None:
            section = self.unqS[0]
        
        # load from drive using fileu
        lbl = fileu.load(os.path.join(self.inputpath,section),type = 'mask',model_type = label_type)
        lbl = lbl.numpy()
        # zero out any labels in the mask do not mattch the TG names
        # does that by finding layers, getting names, subsetting to section
        names = self.Layers[0].names
        Sections = self.Layers[0].Section
        ix = np.flatnonzero(Sections == section)
        names = names[ix]
        # find all unique labels in the lbl matrix
        i, j = np.nonzero(lbl)
        unq_lbl = np.unique(lbl[i,j])
        unq_lbl_flt = np.copy(unq_lbl)
        unq_names = np.unique(names)
        ix_flt = np.logical_not(np.isin(unq_lbl_flt,unq_names))
        unq_lbl_flt[ix_flt]=0
        
        # zeros out labels that are not in names
        lookup_o2n = pd.Series(unq_lbl_flt, index=unq_lbl)
        lbl_filtered = geomu.swap_mask(lbl, lookup_o2n)

        if flip:
           lbl_filtered = np.transpose(lbl_filtered) 

        return(lbl_filtered)

class TissueGraph:
    """Representation of transcriptional state of biospatial units as a graph. 
    
    TissueGraph (TG) is the core class used to analyze tissues using Graph representation. 
    TG stores a two layer graph G = {V,Es,Ef} where Es are spatial edges (physical neighbors) and Ef are feature neighnors. 
    TG stores information on position (XYS) and features that used to created these graphs using anndata object representation.  
    Each TG has a reference to a Taxonomy object that contains information on the types ( 
    
    Note
    ----
    TissueGraph objects are typically not created on their own, but using method calls of TMG (create_{cell,isozones,regions}_layer)
    
    Attributes
    ----------
        tax : Taxonomy
            a Taxonomy object that contain labels, type stats, and relationship between types. 
        SG : iGraph
            Graph representation of the spatial relationship between biospatiual units (i.e. cells, zones, regions). 
            SG might include multiple componennts for multiple section data and is non-weighted graph (1 - neighbors, 0 not neighbors). 
        FG : iGraph 
            Graph representation of feature similarity between biospatial units.  
               
        
    main methods:
        * contract_graph - find zones/region, i.e. spatially continous areas in the graph the same (cell/microenvironment) type
        * cond_entopy - calculates the conditional entropy of a graph given types (or uses defaults existing types)
        * watershed - devide into regions based on watershed


    """
    def __init__(self, basepath=None, layer_type=None, dataset=None,
                       feature_mat=None, feature_mat_raw=None,
                       redo=False, obs=None,
                       quick_load_cell_obs=False
        ):
        """Create a TissueGraph object
        
        Parameters
        ----------
        feature_mat : numpy 2D arrary or tuple of matrix size
            Matrix of the spatial units features (expression, composition, etc). 
            As alternative to the full matrix, input could be a tuple of matrix size (samples x features)
        basepath : str
            Where to read/write files related to this TG
        layer_type : str
            Name of the type of layer (cells, isozones, regions)
        """
        self.allowed_layer_types = ['cell', 'isozone', 'region']

        # validate input
        if basepath is None: 
            raise ValueError("missing basepath in TissueGraph constructor")
        if dataset is None: 
            raise ValueError("missing dataset in TissueGraph constructor")
        if layer_type is None: 
            raise ValueError("Missing layer type information in TissueGraph constructor")
        if layer_type not in self.allowed_layer_types:
            raise ValueError(f"Layer type {layer_type} is not in {self.allowed_layer_types}")

        # what is stored in this layer (cells, zones, regions, etc)
        self.layer_type = layer_type # label layers by their type
        self.basepath = basepath
        self.dataset = dataset
 
        if feature_mat is None and not fileu.check_existance(path=self.basepath,
                                                             type='Layer',
                                                             model_type=self.layer_type,
                                                             dataset=self.dataset):
            raise ValueError(f"either anndata file exists in basepath, or feature_mat must be provided")

           
        # this dict stores the defaults field names in the anndata objects that maps to TissueGraph properties
        # this allows storing different versions (i.e. different cell type assignment) in the anndata object 
        # while still maintaining a "clean" interfact, i.e. i can still call for TG.Type and get a type vector without 
        # knowing anything about anndata. 
        # To see where in AnnData everything is stored, check comment in rows below 
        self.adata_mapping = {"Type": "Type", #obs
                              "node_size": "node_size", #obs
                              "name" : "label", #obs
                              "XY" : "XY", #obsm
                              "Section" : "Slice"} #obs
        # Note: a these mapping are not used for few attributes such as SG/FG/Upstream that are "hard coded" 
        # as much as possible, the only memory footprint is in the anndata object, the exceptions are SG/FG that 
        # are large objects that we want to keep as iGraph in mem. Therefore, SG/FG are created during init from adata.obsp
        
        # Key graphs - spatial and feature based
        self.SG = None # spatial graph (created by build_spatial_graph, or load in __init__)
        self.FG = None # Feature graph (created by build_feature_graph, or load in __init__)

        # if anndata file exists and redo is False, just read the file. 

        # there are three mode of TG loading: 
        # 1. quick (from drive) : only load data that came from Processing (feature_mat and obs), if there are any spatial/feature graphs remove them. 
        # 2. standard (from drive): load the full adata and create SG and FG 
        # 3. from scratch : uses input arguments to rebuild the TG object from scratch, ignores anything in the drive. 
        
        if quick_load_cell_obs:
            self.adata = fileu.load(path=self.basepath,
                                    type='Layer',
                                    model_type=self.layer_type,
                                    dataset=self.dataset)
            self.adata.obsp.clear()

        elif not redo and fileu.check_existance(path=self.basepath,
                                    type='Layer',
                                    model_type=self.layer_type,
                                    dataset=self.dataset):
            self.adata = fileu.load(path=self.basepath,
                                    type='Layer',
                                    model_type=self.layer_type,
                                    dataset=self.dataset)
            # SG is saved as a list of spatial graphs (one per section)
            if "SG" in self.adata.obsp.keys():
                # create SG and FG from Anndata
                sg = self.adata.obsp["SG"] # csr matrix
                self.SG =  tmgu.adjacency_to_igraph(sg, directed=False)
                
            # FG - there is one feature graph for the whole TG object
            if "FG" in self.adata.obsp.keys():
                fg = self.adata.obsp["FG"] # csr matrix
                self.FG = tmgu.adjacency_to_igraph(fg, directed=False)
            
        else: # create an object from given feature_mat data
            # validate input
            if feature_mat is None: 
                raise ValueError("Missing feature_mat in TisseGraph constructor")

            # if feautre_mat is a tuple, replace with NaNs
            if isinstance(feature_mat,tuple): 
                feature_mat = scipy.sparse.csr_matrix(feature_mat)

            # The main data container is an anndata, initalize with feature_mat  
            self.adata = anndata.AnnData(feature_mat, obs=obs,dtype=feature_mat.dtype) # the tissuegraph AnnData object

            if feature_mat_raw is not None:
                self.adata.layers['raw'] = feature_mat_raw
            
            
        return None
    
    def is_empty(self):
        """Determines if the TG object is empty
        
        Checks if internal adata is None of empty
        """ 
        if self.adata is None or self.adata.shape[0]==0: 
            return True
        else: 
            return False

    def filter(self,logical_vec,rebuild_SG = True, rebuild_FG = True): 
        """
        removes observations from TG. 
        can only work if layer_type==cells
        """
        if self.layer_type != "cell": 
            raise("can only filter at the cell level")

        self.adata = self.adata[logical_vec]

        # rebuild igraph layers (that are only saved as adj matrix within anndata)
        if rebuild_SG and self.SG != None: 
            self.build_spatial_graph()
        else:
            # after filtering, need to rebuild SG, if it existed (and rebuild_SG was False) zeros it out
            self.SG = None
        
        if rebuild_FG and self.FG != None: 
            self.build_feature_graph()
        else: 
            # after filtering, need to rebuild FG, if it existed (and rebuild_FG was False) zeros it out
            self.FG = None

    def save(self):
        """save TG to file"""
        if not self.is_empty():
            fileu.save(self.adata,path=self.basepath,
                                  type='Layer',
                                  model_type=self.layer_type,
                                  dataset=self.dataset)
    
    @property
    def names(self):
        """list : observation names"""
        if self.is_empty():
            return None
        return self.adata.obs[self.adata_mapping["name"]]
    
    def get_names(self,section = None):
        if section is None: 
            return(self.names)
        else:
            return(self.names[self.Section == section])

    @names.setter
    def names(self,names):
        self.adata.obs[self.adata_mapping["name"]]=names
    
    @property
    def Upstream(self):
        """list : mapping between current TG layer (self) and upstream layer
        Return value has the length of upstream level and index values of current layer""" 
        if self.is_empty():
            return None
        # Upstream is stored as uns in adata: 
        return self.adata.uns["Upstream"]
    
    @Upstream.setter
    def Upstream(self,V):
        self.adata.uns["Upstream"]=V
    
    @property
    def feature_mat(self):
        """matrix : the feature values for this TG observations
        
        The feature_mat is stored in the underlying anndata object and is required to properly init it. 
        """
        # if adata is still None, return None
        if self.is_empty():
            return None
        # otherwide, feature_mat is stored as the main data in adata
        return(self.adata.X)
    
    @feature_mat.setter
    def feature_mat(self,X):
        self.adata.X = X
    
    def get_feature_mat(self,section = None):
        if section is None: 
            return(self.feature_mat)
        else: 
            return(self.feature_mat[self.Section == section,:])

    @property 
    def Type(self): 
        """Type
        """
        if self.is_empty():
            return None
        elif self.adata_mapping["Type"] not in self.adata.obs.columns.values.tolist(): 
            return None
            # raise ValueError("Mapping of type to AnnData is broken, please check!")
        else:
            typ = self.adata.obs[self.adata_mapping["Type"]]
            typ = np.array(typ) 
            return typ
        
    @Type.setter
    def Type(self,Type):
        """list : list (or 1D np array) of integer values that reference a Taxonomy object types""" 
        self.adata.obs[self.adata_mapping["Type"]] = Type
        
    @property
    def N(self):
        """int : Size of the tissue graph
            internally stored as igraph size
        """
        if not self.is_empty():
            return(self.adata.shape[0])
        else: 
            raise ValueError('TissueGraph does not contain an AnnData object, please verify!')
    
    @property
    def node_size(self):
        if self.is_empty():
            return None
        elif self.adata_mapping["node_size"] not in self.adata.obs.columns.values.tolist(): 
            raise ValueError("Mapping of type to AnnData is broken, please check!")
        else: 
            return self.adata.obs[self.adata_mapping["node_size"]]
    
    @node_size.setter
    def node_size(self,Nsz):
        self.adata.obs[self.adata_mapping["node_size"]] = list(Nsz)
    
    @property
    def Section(self):
        """
            Section : dependent property - will query info from anndata and return
        """
        if self.adata is None:
            return None
        elif self.adata_mapping["Section"] not in self.adata.obs.columns.values.tolist(): 
            raise ValueError("Mapping of type to AnnData is broken, please check!")
        else: 
            return self.adata.obs[self.adata_mapping["Section"]]

    @property
    def unqS(self):
        Sections = self.Section
        # return a list of (unique) section
        return(list(np.unique(Sections)))

    @property
    def size_of_sections(self):
        unq,count = np.unique(self.Section,return_counts = True)
        return(count)

    @Section.setter
    def Section(self,Section):
        self.adata.obs[self.adata_mapping["Section"]]=Section

    @property
    def XY(self):
        """
            XY : dependent property - will query info from anndata and return
        """
        if self.adata is None:
            return None
        elif self.adata_mapping["XY"] not in self.adata.obsm.keys(): 
            raise ValueError("Mapping of XY to AnnData is broken, please check!")
        else: 
            return self.adata.obsm[self.adata_mapping["XY"]]

    def get_XY(self,section = None):
        if section is None: 
            return(self.XY)
        else: 
            return(self.XY[self.Section == section,:])

    @XY.setter
    def XY(self,XY): 
        self.adata.obsm[self.adata_mapping["XY"]]=XY
        
    @property    
    def X(self):
        """
            X : dependent property - will query info from Graph and return
        """
        return(self.XY[:,0])
        
    @property
    def Y(self):
        """Y : dependent property - will query info from Graph and return
        """
        return(self.XY[:,1])
    
    @property    
    def Ntypes(self): 
        """ 
            Ntypes: returns number of unique types in the graph
        """ 
        if self.Type is None: 
            raise ValueError("Type not yet assigned, can't count how many")
        return(len(np.unique(self.Type)))
    
    @property
    def Nsections(self):
        """
            Nsections : returns number of unique sections in TG
        """
        unqS = np.unique(self.Section)
        return(len(unqS))
    
    def build_feature_graph(self, 
        X = None, n_neighbors=15, metric=None, accuracy={'prob':1, 'extras':1.5}, metric_kwds={}, return_graph=False):
        """construct k-graph based on feature similarity

        Create a kNN graph (an igraph object) based on feature similarity. The core of this method is the calculation on how to find neighbors. 
        If metric is "precomputed" the distances are assumed to be known and we're almost done. 
        For all other metric values, we use pynndescent 

        Parameters
        ----------
        X : numpy array
            Either a distance matrix, i.e. squareform(pdist(feature_mat)) if metric = 'precomputed'.  ) or just a feature_mat
            by defaults (if it's None) will use self.feature_mat
        n_neighbors : int
            How many neighbors (k) should we use in the knn graph
        metric : str
            either "precomputed", "random", or one of the MANY metrics supported by pynndescent. Random is for debugging only. 
        accuracy : dict with fields: 'prob' and 'extras'
            a dictionary with accuracy options for pynndescent. 'prob' should be in [0,1] and 'extras' is typically >1. 
            accuracy['prob'] conrols the 'diversify_prob' and accuracy['extra'] the 'pruning_degree_multplier' 
        metric_kwds : dict
            passthrough kwds that will be sent to pynndescent. 
        return_graph : bool
            will return the graph instead of updating self.FG

        Note
        ----
        There are LOTS of metric implemnted in pynndescent. 
        Many are not updated in the readthedocs so check the sources code! 
        """
    
        logging.info(f"building feature graph using {metric}")
        if metric is None:
            raise ValueError('metric was not specified')

        # If X is none, use feature_mat
        if X is None: 
            X = self.feature_mat

        # checks if we have enough rows 
        n_neighbors = min(X.shape[0]-1,n_neighbors)

        if metric == 'precomputed':
            indices = np.argsort(X,axis=1)
            distances = np.sort(X,axis=1)
        elif metric == 'random': 
            indices = np.random.randint(X.shape[0],size=(X.shape[0],n_neighbors+1))
            distances = np.ones((X.shape[0],n_neighbors+1))
        else:
            # perform nn search (using accuracy x number of neighbors to improve accuracy)
            knn = pynndescent.NNDescent(X,n_neighbors = n_neighbors,
                                          metric = metric,
                                          diversify_prob = accuracy['prob'],
                                          pruning_degree_multiplier = accuracy['extras'],
                                          metric_kwds = metric_kwds)

            # get indices and remove self. 
            (indices,distances) = knn.neighbor_graph

        # take the first K values remove first self similarities    
        indices = indices[:,1:]
        distances = distances[:,1:]

        id_from = np.tile(np.arange(indices.shape[0]),indices.shape[1])
        id_to = indices.flatten(order='F')

        # build graph
        edgeList = np.vstack((id_from,id_to)).T
        G = igraph.Graph(n=X.shape[0], edges=edgeList)
        G.simplify()

        # register
        self.adata.obsp["FG"] = G.get_adjacency_sparse()
        self.FG = G
        if return_graph:
            return G
    
    def build_spatial_graph(self,max_dist = 300):
        """construct graph based on Delaunay neighbors
        
        build_spatial_graph will create an igrah using Delaunay triangulation
        
        Spatial graph can potentially be a multi-component one. Cells cannot be neighbors if they are in different sections
        or or they are more than max_dist away from each other. 

        """
        unqS = self.unqS
        logging.info(f"Building spatial graphs for {self.Nsections} sections")
        
        self.SG = list()
        csr_list = list()
        for s in range(self.Nsections): 
            # get XY for a given section
            XY_per_section = self.XY[self.Section==unqS[s],:]
            self.SG.append(geomu.spatial_graph_from_XY(XY_per_section,max_dist=max_dist))
            
        # to merge the spatial graphs into one with many components: 
        self.SG = igraph.Graph.disjoint_union(self.SG[0],self.SG[1:])
        
        logging.info("updating anndata")
        self.adata.obsp["SG"] = self.SG.get_adjacency_sparse()
        self.adata.obs[self.adata_mapping["node_size"]] = np.ones(self.XY.shape[0])
        logging.info("done building spatial graph")
    
    def contract_graph(self, TypeVec=None):
        """find zones/region, i.e. spatially continous areas in the graph the same (cell/microenvironment) type
        
        reduce graph size by merging spatial neighbors of same type. 
        Given a vector of types, will contract the graph to merge vertices that are both next to each other and of the same type. 
        
        to deal with sections, i.e. discontinous spatial graphs, it first merges the SG list into a large grpah 
        does all the calculations, and then splits it again. 

        Parameters
        ----------
        TypeVec : 1D numpy array with dtype int (default value is self.Type)
            a vector of Types for each node. If None, will use self.Type

        Note
        ----
        Default behavior is to assign the contracted TG the same taxonomy as the original graph. 
        
        Returns
        -------
        TissueGraph 
            A TG object after vertices merging. 
        """

        # Figure out which type to use
        if TypeVec is None: 
            TypeVec = self.Type

        # Merge the spatial graphs from multiple sections into one large 
        
        # get edge list - note that Spatial graphs work with indexes not cell names      
        EL = np.asarray(self.SG.get_edgelist()).astype("int")
        
        # only keep edges where neighbors are of same types
        EL = EL[np.take(TypeVec,EL[:,0]) == np.take(TypeVec,EL[:,1]),:]
        
        # remake a graph with potentially many components
        IsoZonesGraph = igraph.Graph(n=self.N, edges=EL, directed = False)
        IsoZonesGraph = IsoZonesGraph.as_undirected().simplify()

        # because we used both type and proximity, the original graph (based only on proximity)
        # that was a single component graph will be broken down to multiple components 
        # finding clusters for each component. 
        cmp = IsoZonesGraph.components()
        
        IxMapping = np.asarray(cmp.membership)
        
        ZoneName, ZoneSingleIx = np.unique(IxMapping, return_index=True)
        
        # zone size sums the current graph zone size per each aggregate (i.e. zone or microenv)
        df = pd.DataFrame(data = self.node_size)
        df['type'] = IxMapping
        ZoneSize = df.groupby(['type']).sum()
        ZoneSize = np.array(ZoneSize).flatten()
        
        # calculate zones feature_mat
        # if all values are Nan, just replace with tuple of the required size
        if np.all(np.isnan(self.feature_mat)): 
            zone_feat_mat = (len(ZoneSize),self.feature_mat.shape[1])
        else: 
            df = pd.DataFrame(data = self.feature_mat)
            df['type']=IxMapping
            zone_feat_mat = np.array(df.groupby(['type']).mean())
            
        # create new SG for zones 
        ZSG = self.SG.copy()
        ZSG.contract_vertices(IxMapping)
        ZSG.simplify()

        # create a new Tissue graph by copying existing one, contracting, and updating XY
        ZoneGraph = TissueGraph(feature_mat=zone_feat_mat, 
                                basepath=self.basepath,
                                dataset=self.dataset,
                                layer_type="isozone",
                                redo=True,
                                )
        
        # recreate contracted graph as a TissueGraph 
        unq_ix = np.unique(IxMapping)
        new_XY = np.zeros((len(unq_ix),2))
        new_section = []
        for i,qix in enumerate(unq_ix):
            ix = np.flatnonzero(IxMapping==qix)
            new_XY[i,:] = self.XY[ix,:].mean(axis=0)
            new_section.append(self.Section[ix[0]])

        ZoneGraph.XY = new_XY
        ZoneGraph.Section = new_section
        ZoneGraph.SG = ZSG
        ZoneGraph.names = ZoneName
        ZoneGraph.node_size = ZoneSize
        ZoneGraph.Type = TypeVec[ZoneSingleIx]
        ZoneGraph.Upstream = IxMapping
        
        return(ZoneGraph)
                             
    def type_freq(self): 
        """return the catogorical probability for each type in TG
        
        Probabilities are weighted by the node_size
        """
        if self.Type is None: 
            raise ValueError("Type not yet assigned, can't count frequencies")
        unqTypes = np.unique(self.Type)
        Ptypes = tmgu.count_values(self.Type,unqTypes,self.node_size)
        
        return Ptypes,unqTypes
    
    def cond_entropy(self):
        """calculate conditional entropy of the tissue graph
           
           cond entropy is the difference between graph entropy based on zones and type entropy
        """
        Pzones = self.node_size
        Pzones = Pzones/np.sum(Pzones)
        Entropy_Zone = -np.sum(Pzones*np.log2(Pzones))
        
        # validate that type exists
        if self.Type is None: 
            raise ValueError("Can't calculate cond-entropy without Types, please check")
            
        Ptypes = self.type_freq()[0] 
        Entropy_Types=-np.sum(Ptypes*np.log2(Ptypes))
        
        cond_entropy = Entropy_Zone-Entropy_Types
        return(cond_entropy)
    
    def extract_environments(self,ordr = None,typevec = None):
        """returns the categorical distribution of neighbors. 
        
        Depending on input there could be two uses, 
            usage 1: if ordr is not None returns local neighberhood defined as nodes up to distance ordr on the graph for all vertices. 
            usage 2: if typevec is not None returns local env based on typevec, will return one env for each unique type in typevec
            
        Return
        ------
        numpy array
            Array with Type frequency for all local environments for all types in TG. 
        """
        unqlbl = np.unique(self.Type)
        
        # arrange the indexes for the environments. 
        # if we use ordr this is neighborhood defined by iGraph
        # if we provide types, than indexes of each type. 
        if ordr is not None and typevec is None:
            ind = self.SG.neighborhood(order = ordr)
        elif typevec is not None and ordr is None:
            ind = list()
            for i in range(len(np.unique(typevec))):
                ind.append(np.flatnonzero(typevec==i))
        else: 
            raise ValueError('either order or typevec must be provided, not both (or none)')
        
        unqlbl = np.unique(self.Type)
        Env = np.zeros((len(ind),len(unqlbl)),dtype=np.int64)
        ndsz = self.node_size.copy().astype(np.int64)
        int_types = self.Type.astype(np.int64)
        AllTypes = [int_types[ix] for ix in ind]
        AllSizes = [ndsz[ix] for ix in ind]
        for i in range(Env.shape[0]):
            Env[i,:]=np.bincount(AllTypes[i],weights = AllSizes[i],minlength=len(unqlbl))
        
        # should be the same as above, but much slower... keeping it here for now till more testing is done. 
        # for i in range(len(ind)):
        #     Env[i,:]=count_values(self.Type[ind[i]],unqlbl,ndsz[ind[i]],norm_to_one = False)
        return(Env)
    
    def graph_local_avg(self,VecToSmooth,ordr = 3,kernel = np.array([0.4,0.3,0.2,0.1])):
        """Simple local average of a Vec based on local neighborgood
        
        Parameters
        ----------
            VecToSmooth : numpy array
                The values we want to smooth. len(VecToSmooth) must be self.N
        """
        
        if len(VecToSmooth) is not self.N: 
            raise ValueError(f"Length of input vector {len(VecToSmooth)} doesn't match TG.N which is {self.N}")
        
        Smoothed = np.zeros((len(VecToSmooth),ordr+1))
        Smoothed[:,0] = VecToSmooth
        for i in range(ordr):
            ind = self.SG.neighborhood(order = i+1,mindist=i)
            for j in range(len(ind)):
                ix = np.array(ind[j],dtype=np.int64)
                Smoothed[j,i+1] = np.nanmean(VecToSmooth[ix])

        kernel = kernel[None,:]
        kernel = np.repeat(kernel,len(VecToSmooth),axis=0)
        ix_nan = np.isnan(Smoothed)
        kernel[ix_nan]=0
        sum_of_rows = kernel.sum(axis=1)
        kernel = kernel / sum_of_rows[:, None]
        Smoothed[ix_nan] = 0
        Smoothed = np.multiply(Smoothed,kernel)
        Smoothed = Smoothed.sum(axis=1)
        return(Smoothed)
    
    def watershed(self,InputVec):
        """Watershed segmentation based on InputVec values
        
        Watershed on the TG spatial graph. 
        First finds local peaks and then assigns all nodes to their closest local peak using dijkstra
        
        Parameters
        ----------
        
        Return
        ------
        (id,dist) 
            tuple with id and distance to cloest zone. 
        """
        is_peak = np.zeros(InputVec.shape).astype('bool')
        ind = self.SG.neighborhood(order = 1,mindist=1)
        for i in range(len(ind)):
            is_peak[i] = np.all(InputVec[i]>InputVec[ind[i]])
        peaks = np.flatnonzero(is_peak)  

        Adj = self.SG.get_adjacency_sparse()
        Dij_min, predecessors, ClosestPeak = dijkstra(Adj, directed=False, 
                                                          indices=peaks, 
                                                          return_predecessors=True, 
                                                          unweighted=False, 
                                                          limit=np.inf, 
                                                          min_only=True)
        
        # renumber all closest peak continously
        u,ix_rev = np.unique(ClosestPeak, return_inverse=True)
        ClosestPeak=u[ix_rev]
        
        # relabel HoodId in case we have a heterozone that was split along the way
        # by contracting and expanding where each contracted zone gets a unique ID we
        # basically garantuee that Ntypes = N (for now...)
        CG = self.contract_graph(TypeVec = ClosestPeak)
        Id = np.arange(CG.N)
        ClosestPeak = Id[CG.Upstream]
        return (ClosestPeak, Dij_min)
        
    def calc_entropy_at_different_Leiden_resolutions(self,Rvec = np.logspace(-1,2.5,100)): 
        Ent = np.zeros(Rvec.shape)
        Ntypes = np.zeros(Rvec.shape)
        for i in range(len(Rvec)):
            TypeVec = self.FG.community_leiden(resolution_parameter=Rvec[i],objective_function='modularity').membership
            TypeVec = np.array(TypeVec).astype(np.int64)
            Ent[i] = self.contract_graph(TypeVec).cond_entropy()
            Ntypes[i] = len(np.unique(TypeVec))
            
        self.cond_entropy_df = pd.DataFrame(data = {'Entropy' : Ent, 'Ntypes' : Ntypes, 'Resolution' : Rvec})     
    
    def gradient_magnitude(self,V):
        """Spatial gradient based on spatial graph
        
        Calculate the gradient defined as sqrt(dV/dx^2+dV/dy^2) where dV/dx(y) is calcualted using simple trigo
        
        """
        EL = self.SG.get_edgelist()
        EL = np.array(EL,dtype='int')
        XY = self.XY
        dV = V[EL[:,1]]-V[EL[:,0]]
        dX = XY[EL[:,1],0]-XY[EL[:,0],0]
        dY = XY[EL[:,1],1]-XY[EL[:,0],1]                      
        alpha = np.arctan(dX/dY)
        dVdX = dV*np.cos(alpha)/dX
        dVdY = dV*np.sin(alpha)/dY 
        dVdXY = np.sqrt(dVdX**2 + dVdY**2)
        df = pd.DataFrame({'dVdXY' : np.hstack((dVdXY,-dVdXY))})
        df['type']=np.hstack((EL[:,0],EL[:,1]))
        gradmag = np.array(df.groupby(['type']).mean())
        gradmag = np.array(gradmag)
        return(gradmag)
    
    def graph_local_median(self,VecToSmooth,ordr = 3):
        """local median of VecToSmooth based in spatial graph
        """
        ind = self.SG.neighborhood(order = ordr)
        Smoothed = np.zeros(VecToSmooth.shape)
        for j in range(len(ind)):
            ix = np.array(ind[j],dtype=np.int64)
            Smoothed[j] = np.nanmedian(VecToSmooth[ix])
        return(Smoothed)

class Taxonomy:
    """Taxonomical system for different biospatial units (cells, isozones, regions)
    
    Attributes
    ----------
    name : str
        a name of the taxonomy (will be used to file io)
    
    """
    
    def __init__(self, name=None, 
        Types=None, feature_mat=None,rgb_codes=None
        ): 
        """Create a Taxonomy object

        Parameters
        ----------
        name : str
            the name of the taxonomy object
        Types : list 
            list of types names that will be used in this Taxonomy
        """
        if name is None: 
            raise ValueError("Taxonomy must get a name")
        self.name = name

        if Types is not None and feature_mat is not None: 
            self.add_types(Types,feature_mat)

        # add RGB values if provided
        self._df = pd.DataFrame()
        if rgb_codes is not None:     
            self._df['RGB'] = rgb_codes

        return None
    
    @property
    def Type(self): 
        """list: Building blocks of this Taxonomy
        
        Setter verify that there are no duplications. 
        """
        return(list(self._df.index))
    
    @Type.setter
    def Type(self,Types):
        if len(Types) is not len(set(Types)):
            dups = [Types.count(t) for t in set(Types) if Types.count(t)>1]
            raise ValueError(f"Types must be unique. Found duplicates: {dups}")
        if self.is_empty():
            self._df.index = Types
        else: 
            if len(Types) is not self._df.shape[0]: 
                raise ValueError('Changing Types of Taxonomy with defined values is not allowed')
            self._df.index = Types

    @property
    def feature_mat(self): 
        """ndarray: feature values for all types in the taxonomy
        
        Setter can get as input either numpy array (ordered same as self.Type) or pandas dataframe
        """
        if self.is_empty(): 
            return None
        else: 
            return(self._df.to_numpy())
        
    @feature_mat.setter
    def feature_mat(self,F):
        # first, make sure F is a DataFrame
        if isinstance(F, pd.DataFrame):
            df_features = F
        else: # assumes F is a matrix, make into a df and give col names
            df_features = pd.DataFrame(F)
            df_features.columns = [f"f_{i:03d}" for i in range(F.shape[1])]

        self._df = df_features
    #    # either replace or concat columns based on their name
    #     for c in df_features.columns: 
    #         if c in self._feature_cols: 
    #             self._df.loc[:,c]=F.loc[:,c]
    #         else: 
    #             self._df = pd.concat((self._df,df_features.loc[:,c]),axis=1)
    #             self._feature_cols.append(c)
        
    #     self._feature_cols = df_features.columns 
        
    def is_empty(self):
        """ determie if taxonomy is empty
        """
        return (self._df.empty)
        
    def save(self, basepath,dataset):
        """save to basepath using Taxonomy name
        """
        if not self.is_empty():
            fileu.save(self._df,path=basepath,
                                  type='Taxonomy',
                                  model_type=self.name,
                                  dataset=dataset)
                
    def load(self, basepath,dataset):
        """save from basepath using Taxonomy name
        """
        self._df = fileu.load(path=basepath,
                              type='Taxonomy',
                              model_type=self.name,
                              dataset=dataset)
        self._df.set_index('type', inplace=True)
        
    def add_types(self,new_types,feature_mat):
        """add new types and their feature average to the Taxonomy
        """
        df_new = pd.DataFrame(feature_mat)
        df_new['type']=new_types
        type_feat_df = df_new.groupby(['type']).mean()
        if self._df.empty:
            self._df = type_feat_df
        else: 
            missing_index = type_feat_df.index.difference(self._df.index)
            self._df = pd.concat((self._df,type_feat_df.iloc[missing_index,:]),axis=0)
        
        return None
    
    @property
    def N(self):
        return len(self.Type)

    @property
    def RGB(self):
        """
        Returns RGB values for all types (random value if none assigned)
        """
        if 'RGB' in self._df.columns:
            rgb = self._df['RGB']
        else: 
            rgb = coloru.rand_hex_codes(self._df.shape[0])
        return rgb

    @RGB.setter
    def RGB(self,rgb_codes):
        self._df['RGB'] = rgb_codes

class Geom:
    """
    Container class for a collection of polygons. 
    These geometires could be used to represent the units in a TissueGraphs but other uses are possible. 
    
    Supported Geoms: 
        * voronoi
        * cells
        * nuclei 
        * cytoplasm 
        * isozones 
        * regions 

        NOTE (1/25/2023): apperantly, extracting coordinates from 100K polygons in shapely is pretty slow
        therefore, during construction, doing this calculation once and caching it as _vert. 
        A wrap-around property, vert, serves that. If this could be faster, might make sense to calculate that on the fly. 

        For the vertices representation. Verts is a tuple of (ext,int) one for each polygon. Ext has one polygon and int a list of 
        potentially many "holes". The verts are computed by get_polygons_vertices and by keeping them in memory we are avoiding
        recomputing this every time the geomtry is used (mostly for plotting). This does add one-time cost during 
        __init__ to create this cached precomputed representation. 

    """

    def __init__(self,geom_type = '',polys = [],basepath = '',dataset = '',section = ''):
        self.type = geom_type
        self.allowed_geom_types=['voronoi',
                                  'cells',
                                  'nuclei',
                                  'cytoplasm',
                                  'isozones',
                                  'regions',
                                  'mask']
        if geom_type not in self.allowed_geom_types:
            raise ValueError(f"Geom type {geom_type} is not in {self.allowed_geom_types}")
        self.polys = polys
        self._verts = geomu.get_polygons_vertices(polys)
        self.basepath = basepath
        self.dataset = dataset
        self.section = section
        pass

    @property
    def verts(self):
        return self._verts

    def save(self):
        fileu.save(self.polys,path = self.basepath,type = 'Geom',
                   model_type = self.type,dataset = self.dataset,
                   section = self.section)
        pass 

    def get_xylim(self):
        """Returns the min/max xy of the geometry
        """
        # get xy of exterior coordinates
        xy = np.vstack(self.verts[0])
        # get limits
        xlim = [xy[:,0].min(), xy[:,0].max()]
        ylim = [xy[:,1].min(), xy[:,1].max()]
        return xlim,ylim
        

    